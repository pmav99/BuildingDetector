{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "In this notebook, I'm going to try getting rid of the template matching step of my building finder and instead just have a sliding window that checks to see if there are roofs in each window. Doing it this way (with constant candidate patch sizes), I can also use the built-in HOG features.\n",
    "\n",
    "The first step will be to grab a bunch of image samples and pull out roof (positive) and nonroof (negative) training examples. Hopefully we can get a somewhat balanced training set.\n",
    "\n",
    "The next step will be to get a set of representative images randomly sampled from the DHS locations and to try to classify each window within them. In this step, it will be useful to save the image patches classified as roofs and nonroofs in separate folders so that we can identify cases where misclassification occurred. We can then add these hard cases to the training set and hopefully improve our classification algorithm.\n",
    "\n",
    "We can repeat this process until we (hopefully) get a good image patch roof/nonroof classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import urllib\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from features import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 5 images from DHS cell 100498 in 1.83570694923 seconds.\n",
      "Sampled 5 images from DHS cell 101916 in 1.93134522438 seconds.\n",
      "Sampled 5 images from DHS cell 101940 in 1.91911292076 seconds.\n",
      "Sampled 5 images from DHS cell 101946 in 1.83732390404 seconds.\n",
      "Sampled 5 images from DHS cell 101976 in 1.85520505905 seconds.\n",
      "Sampled 5 images from DHS cell 103383 in 2.00206303596 seconds.\n",
      "Sampled 5 images from DHS cell 104103 in 2.01003313065 seconds.\n",
      "Sampled 5 images from DHS cell 106271 in 3.28763699532 seconds.\n",
      "Sampled 5 images from DHS cell 106977 in 2.25710320473 seconds.\n",
      "Sampled 5 images from DHS cell 106991 in 1.93952918053 seconds.\n",
      "Sampled 5 images from DHS cell 109148 in 2.0437078476 seconds.\n",
      "Sampled 5 images from DHS cell 110577 in 1.96419787407 seconds.\n",
      "Sampled 5 images from DHS cell 110578 in 2.4681019783 seconds.\n",
      "Sampled 5 images from DHS cell 111296 in 2.45287013054 seconds.\n",
      "Sampled 5 images from DHS cell 111297 in 2.56994104385 seconds.\n",
      "Sampled 5 images from DHS cell 112736 in 2.01135993004 seconds.\n",
      "Sampled 5 images from DHS cell 119239 in 1.8899679184 seconds.\n",
      "Sampled 5 images from DHS cell 119959 in 3.0917289257 seconds.\n",
      "Sampled 5 images from DHS cell 123511 in 1.9211909771 seconds.\n",
      "Sampled 5 images from DHS cell 123560 in 5.07299685478 seconds.\n",
      "Sampled 5 images from DHS cell 124259 in 2.63897395134 seconds.\n",
      "Sampled 5 images from DHS cell 124979 in 1.9509999752 seconds.\n",
      "Sampled 5 images from DHS cell 126427 in 2.75122284889 seconds.\n",
      "Sampled 5 images from DHS cell 127100 in 5.97297787666 seconds.\n",
      "Sampled 5 images from DHS cell 127141 in 1.97266888618 seconds.\n",
      "Sampled 5 images from DHS cell 127859 in 1.88355207443 seconds.\n",
      "Sampled 5 images from DHS cell 127874 in 2.00689601898 seconds.\n",
      "Sampled 5 images from DHS cell 127875 in 1.83494901657 seconds.\n",
      "Sampled 5 images from DHS cell 128594 in 1.87150883675 seconds.\n",
      "Sampled 5 images from DHS cell 128595 in 1.89647507668 seconds.\n",
      "Sampled 5 images from DHS cell 129313 in 1.78450584412 seconds.\n",
      "Sampled 5 images from DHS cell 130746 in 1.86254000664 seconds.\n",
      "Sampled 5 images from DHS cell 135740 in 3.78165698051 seconds.\n",
      "Sampled 5 images from DHS cell 135744 in 10.4420719147 seconds.\n",
      "Sampled 5 images from DHS cell 136453 in 1.85128903389 seconds.\n",
      "Sampled 5 images from DHS cell 136454 in 1.97502708435 seconds.\n",
      "Sampled 5 images from DHS cell 136455 in 2.33955001831 seconds.\n",
      "Sampled 5 images from DHS cell 136456 in 3.86670207977 seconds.\n",
      "Sampled 5 images from DHS cell 136457 in 3.70698308945 seconds.\n",
      "Sampled 5 images from DHS cell 137152 in 4.04144906998 seconds.\n",
      "Sampled 5 images from DHS cell 137157 in 4.10356688499 seconds.\n",
      "Sampled 5 images from DHS cell 137172 in 8.91672897339 seconds.\n",
      "Sampled 5 images from DHS cell 137173 in 2.2791030407 seconds.\n",
      "Sampled 5 images from DHS cell 137174 in 2.79602718353 seconds.\n",
      "Sampled 5 images from DHS cell 137175 in 2.12295889854 seconds.\n",
      "Sampled 5 images from DHS cell 137176 in 2.057721138 seconds.\n",
      "Sampled 5 images from DHS cell 137177 in 3.69057703018 seconds.\n",
      "Sampled 5 images from DHS cell 137879 in 2.05503606796 seconds.\n",
      "Sampled 5 images from DHS cell 137880 in 1.86029601097 seconds.\n",
      "Sampled 5 images from DHS cell 137892 in 2.76905703545 seconds.\n",
      "Done. Sampled 250 images total.\n"
     ]
    }
   ],
   "source": [
    "# Import and preview csv data\n",
    "dhs_fn = '../data/IMR1990-2000_NL1992-2012_thresh100.csv'\n",
    "dhs_data = pd.read_csv(dhs_fn)\n",
    "\n",
    "# Samples to take from each location\n",
    "samples = 5\n",
    "# Number of locations to sample from\n",
    "locations = 50\n",
    "\n",
    "# Output image directory\n",
    "out_dir = '../images/forest/samples/'\n",
    "out_csv = '../data/dhs_image_metadata.csv'\n",
    "\n",
    "# Create DataFrame for image metadata\n",
    "image_data = pd.DataFrame(columns=['image', 'cellid', 'cell_lat',\n",
    "                                  'cell_lon', 'lat', 'lon'])\n",
    "\n",
    "# Sampling images from DHS locations\n",
    "for index, row in dhs_data.iterrows():\n",
    "    if index + 1 > locations:\n",
    "        break\n",
    "    \n",
    "    cell_id = int(row['cellid'])\n",
    "    cell_lat = row['lat']\n",
    "    cell_lon = row['lon']\n",
    "    # Sample images\n",
    "    image_data = sample_dhs(image_data, cell_id, cell_lat, cell_lon,\n",
    "                           samples, out_dir)\n",
    "\n",
    "# Save image metadata to csv\n",
    "image_data.set_index('image', inplace=True)\n",
    "image_data.to_csv(out_csv)\n",
    "\n",
    "print 'Done. Sampled {} images total.'.format(samples * locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 974 images in 0.525631904602 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Rename images first\n",
    "in_dir = '../images/forest/samples/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = in_dir + str(count) + '.jpg'\n",
    "    os.rename(image_fn, out_fn)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Renamed {} images in {} seconds.'.format(count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have some images, we need to split them up into 81 (9x9) 80x80 pixel patches. Let's write a few functions to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_patch(image, out_fn, x=0, y=0, width=80, height=80):\n",
    "    \"\"\"\n",
    "    This function saves a specified image patch from a image.\n",
    "    :param image: Input 3-channel image\n",
    "    :param out_fn: Filename for output image patch\n",
    "    :param x: Top left x-coordinate of image patch\n",
    "    :param y: Top left y-coordinate of image patch\n",
    "    :param width: Width in pixels of image patch\n",
    "    :param height: Height in pixels of image patch\n",
    "    \"\"\"\n",
    "    patch = image[x:x+width, y:y+width, :]\n",
    "    cv2.imwrite(out_fn, patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_patches(image_fn, out_dir, width=80, height=80):\n",
    "    \"\"\"\n",
    "    This function takes each input image and saves however many image\n",
    "    patches of the specified size as can fit in the original image. \n",
    "    Image patches are offset by half the height and width specified.\n",
    "    :param image_fn: Input image filename\n",
    "    :param out_dir: Folder where image patches will be saved\n",
    "    :param width: Width of each image patch\n",
    "    :param height: Height of each image patch\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_fn)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            out_fn = (out_dir + os.path.basename(image_fn)[:-4] + '_' +\n",
    "                      str(count) + '.png')\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y, width,\n",
    "                       height)\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we can save 81 patches from any image, let's go through our samples and save 81 patches for each of them to separate into positive and negative training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patches for 142 images.\n"
     ]
    }
   ],
   "source": [
    "# Save patches from all sample images\n",
    "in_dir = '../images/forest/samples/'\n",
    "out_dir = '../images/forest/patches/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    save_patches(image_fn, out_dir)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Saved patches for {} images in {} seconds.'.format(count,(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 3048 images in 4.37710499763 seconds.\n",
      "Renamed 3058 images in 4.44911384583 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Rename images first\n",
    "in_dir = '../images/forest/training/roof/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = image_fn + '_abc.png'\n",
    "    os.rename(image_fn, out_fn)\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = in_dir + str(count) + '.png'\n",
    "    os.rename(image_fn, out_fn)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Renamed {} images in {} seconds.'.format(count, (t1-t0))\n",
    "# Rename images first\n",
    "in_dir = '../images/forest/training/nonroof/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = image_fn + '_abc.png'\n",
    "    os.rename(image_fn, out_fn)\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = in_dir + str(count) + '.png'\n",
    "    os.rename(image_fn, out_fn)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Renamed {} images in {} seconds.'.format(count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from training examples\n",
    "Ok, now that we've organized those patches into roof and nonroof classes, let's first use the feature extractor functions that we wrote earlier and see how well it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images for roof class.\n",
      "Processed 200 images for roof class.\n",
      "Processed 300 images for roof class.\n",
      "Processed 400 images for roof class.\n",
      "Processed 500 images for roof class.\n",
      "Processed 600 images for roof class.\n",
      "Processed 700 images for roof class.\n",
      "Processed 800 images for roof class.\n",
      "Processed 900 images for roof class.\n",
      "Processed 1000 images for roof class.\n",
      "Processed 1100 images for roof class.\n",
      "Processed 1200 images for roof class.\n",
      "Processed 1300 images for roof class.\n",
      "Processed 1400 images for roof class.\n",
      "Processed 1500 images for roof class.\n",
      "Processed 1600 images for roof class.\n",
      "Processed 1700 images for roof class.\n",
      "Processed 1800 images for roof class.\n",
      "Processed 1900 images for roof class.\n",
      "Processed 2000 images for roof class.\n",
      "Processed 2100 images for roof class.\n",
      "Processed 2200 images for roof class.\n",
      "Processed 2300 images for roof class.\n",
      "Processed 2400 images for roof class.\n",
      "Processed 2500 images for roof class.\n",
      "Processed 2600 images for roof class.\n",
      "Processed 2700 images for roof class.\n",
      "Processed 2800 images for roof class.\n",
      "Processed 2900 images for roof class.\n",
      "Processed 3000 images for roof class.\n",
      "Processed 3048 images for roof class.\n",
      "Processed 100 images for nonroof class.\n",
      "Processed 200 images for nonroof class.\n",
      "Processed 300 images for nonroof class.\n",
      "Processed 400 images for nonroof class.\n",
      "Processed 500 images for nonroof class.\n",
      "Processed 600 images for nonroof class.\n",
      "Processed 700 images for nonroof class.\n",
      "Processed 800 images for nonroof class.\n",
      "Processed 900 images for nonroof class.\n",
      "Processed 1000 images for nonroof class.\n",
      "Processed 1100 images for nonroof class.\n",
      "Processed 1200 images for nonroof class.\n",
      "Processed 1300 images for nonroof class.\n",
      "Processed 1400 images for nonroof class.\n",
      "Processed 1500 images for nonroof class.\n",
      "Processed 1600 images for nonroof class.\n",
      "Processed 1700 images for nonroof class.\n",
      "Processed 1800 images for nonroof class.\n",
      "Processed 1900 images for nonroof class.\n",
      "Processed 2000 images for nonroof class.\n",
      "Processed 2100 images for nonroof class.\n",
      "Processed 2200 images for nonroof class.\n",
      "Processed 2300 images for nonroof class.\n",
      "Processed 2400 images for nonroof class.\n",
      "Processed 2500 images for nonroof class.\n",
      "Processed 2600 images for nonroof class.\n",
      "Processed 2700 images for nonroof class.\n",
      "Processed 2800 images for nonroof class.\n",
      "Processed 2900 images for nonroof class.\n",
      "Processed 3000 images for nonroof class.\n",
      "Processed 3058 images for nonroof class.\n",
      "Processed 6106 images total in 1066.85848689 seconds.\n"
     ]
    }
   ],
   "source": [
    "sample_dir = '../images/forest/training/'\n",
    "csv_out = '../data/forest_training_data.csv'\n",
    "store_image_data(sample_dir, csv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load those features back into the workspace so that we can use them for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonroof' 'roof']\n",
      "Got class labels for 6106 training data points.\n",
      "Got feature vectors for 6106 training data points.\n",
      "(6106, 473)\n",
      "(6106, 48)\n",
      "(6106, 153)\n",
      "(6106, 272)\n"
     ]
    }
   ],
   "source": [
    "csv_in = '../data/forest_training_data.csv'\n",
    "(features, colors, hogs, mags, labels, label_encoder) = \\\n",
    "                import_image_data(csv_in, display=True)\n",
    "print features.shape\n",
    "print colors.shape\n",
    "print hogs.shape\n",
    "print mags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure none of features are nan\n",
    "np.nonzero(np.isnan(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well a random forest classifier does on these training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mix up the data\n",
    "perm = np.random.permutation(labels.size)\n",
    "features = features[perm]\n",
    "labels = labels[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall classification accuracy: 0.826899128269\n",
      "Took 1.51021814346 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "#clf1 = ensemble.RandomForestClassifier()\n",
    "#clf = ensemble.AdaBoostClassifier(base_estimator=clf1, n_estimators=50,\n",
    "#                                  random_state=0)\n",
    "# Training on training examples\n",
    "num_train = 4500\n",
    "clf.fit(features[:num_train], labels[:num_train])\n",
    "accuracy = clf.score(features[num_train:], labels[num_train:])\n",
    "print 'Overall classification accuracy: {}'.format(accuracy)\n",
    "t1 = time.time()\n",
    "print 'Took {} seconds.'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606 test examples: 828 positive, 778 negative\n",
      "Predicted: 816 positive, 790 negative.\n",
      "Prediction results:\n",
      "    683 true positive, 133 false positive\n",
      "    645 true negative, 145 false negative\n",
      "Roof accuracy: 0.824879227053\n",
      "Nonroof accuracy: 0.829048843188\n"
     ]
    }
   ],
   "source": [
    "# Figuring out the true/false positive/negative rates\n",
    "y_hat = clf.predict(features[num_train:])\n",
    "y = labels[num_train:]\n",
    "num_test = y_hat.shape[0]\n",
    "positive = sum(y)\n",
    "negative = num_test - positive\n",
    "print '{} test examples: {} positive, {} negative'.format(num_test,\n",
    "                                                     positive, negative)\n",
    "positive_hat = sum(y_hat)\n",
    "negative_hat = num_test - positive_hat\n",
    "print 'Predicted: {} positive, {} negative.'.format(positive_hat,\n",
    "                                                   negative_hat)\n",
    "# Different types of mistakes:\n",
    "# 0 = correct, -1 = false positive, 1 = false negative\n",
    "mistakes = y - y_hat\n",
    "false_neg = mistakes > 0\n",
    "false_pos = mistakes < 0\n",
    "false_neg_count = sum(false_neg)\n",
    "false_pos_count = sum(false_pos)\n",
    "true_pos = positive_hat - false_pos_count\n",
    "true_neg = negative_hat - false_neg_count\n",
    "print 'Prediction results:'\n",
    "print '    {} true positive, {} false positive'.format(true_pos,\n",
    "                                                      false_pos_count)\n",
    "print '    {} true negative, {} false negative'.format(true_neg,\n",
    "                                                      false_neg_count)\n",
    "print 'Roof accuracy: {}'.format(float(true_pos) / positive)\n",
    "print 'Nonroof accuracy: {}'.format(float(true_neg) / negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.61740700e-03,   1.40124740e-02,   2.23619795e-02,\n",
       "         2.25458620e-03,   5.91171560e-03,   5.69672774e-03,\n",
       "         7.78562920e-02,   5.88437106e-03,   6.90711081e-03,\n",
       "         5.41748821e-03,   7.55981713e-03,   3.72416211e-03,\n",
       "         2.77912253e-03,   3.33501123e-03,   2.35339679e-03,\n",
       "         1.44054612e-03,   1.54040659e-03,   6.19043214e-02,\n",
       "         2.12452897e-03,   2.66616985e-03,   4.46248002e-03,\n",
       "         9.72824692e-02,   1.13042547e-01,   6.43764587e-02,\n",
       "         4.79664851e-03,   1.63830894e-02,   8.03954521e-02,\n",
       "         3.95437029e-03,   3.40988065e-03,   7.32883046e-03,\n",
       "         2.29866263e-03,   1.44765622e-03,   1.68422994e-03,\n",
       "         2.07998725e-03,   2.47861491e-03,   4.10751331e-03,\n",
       "         7.45372778e-03,   7.43321135e-03,   5.16708571e-02,\n",
       "         1.26738510e-02,   3.76847719e-03,   4.40803269e-03,\n",
       "         5.15455486e-03,   4.14848307e-03,   4.88103367e-03,\n",
       "         3.17352841e-03,   2.08088113e-03,   1.30815643e-03,\n",
       "         3.70487363e-02,   4.10718915e-02,   3.12347180e-02,\n",
       "         7.43593257e-03,   2.52910400e-02,   6.47134889e-03,\n",
       "         5.48831250e-03,   9.27829444e-03,   1.51188875e-02,\n",
       "         1.19455246e-02,   7.90328396e-03,   2.70984783e-02,\n",
       "         1.15697334e-02,   2.47135323e-03,   2.47027597e-03,\n",
       "         1.01387906e-03,   8.28121212e-04,   3.29755230e-04,\n",
       "         4.14751886e-04,   1.31687354e-04,   1.11928217e-04,\n",
       "         4.73839203e-05,   1.49575679e-04,   9.43027153e-06,\n",
       "         3.43560514e-05])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks extremely promising! Let's get some new random DHS images and try to classify the image patches within them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier\n",
    "It seems that this might actually result in a good roof detector! Let's get a new set of DHS images and see how it does classifying all the image patches. We can see where it makes mistakes and then add those \"hard\" examples to the training set--hopefully that will improve the classifier going forward.\n",
    "\n",
    "Ok, we've use the code above to get a new set of DHS images. Let's break those up into 81 image patches each and classify each of them, saving them into separate roof and nonroof folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/samples/'\n",
    "roof_dir = '../images/forest/classify/roof/'\n",
    "nonroof_dir = '../images/forest/classify/nonroof/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='auto', criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest classifier on all training examples\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying patches in image 1.\n",
      "Classifying patches in image 2.\n",
      "Classifying patches in image 3.\n",
      "Classifying patches in image 4.\n",
      "Classifying patches in image 5.\n",
      "Classifying patches in image 6.\n",
      "Classifying patches in image 7.\n",
      "Classifying patches in image 8.\n",
      "Classifying patches in image 9.\n",
      "Classifying patches in image 10.\n",
      "Classifying patches in image 11.\n",
      "Classifying patches in image 12.\n",
      "Classifying patches in image 13.\n",
      "Classifying patches in image 14.\n",
      "Classifying patches in image 15.\n",
      "Classifying patches in image 16.\n",
      "Classifying patches in image 17.\n",
      "Classifying patches in image 18.\n",
      "Classifying patches in image 19.\n",
      "Classifying patches in image 20.\n",
      "Classifying patches in image 21.\n",
      "Classifying patches in image 22.\n",
      "Classifying patches in image 23.\n",
      "Classifying patches in image 24.\n",
      "Classifying patches in image 25.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b08dccf859f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m                                     axis=0)\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# Classify image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 0 = nonroof, 1 = roof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mnonroof_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;31m# ensure_2d=False because there are actually unit test checking we fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;31m# for 1d.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[0;32m    350\u001b[0m                              array.ndim)\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     50\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     51\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 52\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Classify each image patch\n",
    "width = 80\n",
    "height = 80\n",
    "image_count = 0\n",
    "t0 = time.time()\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    # Load in image\n",
    "    image = cv2.imread(image_fn)\n",
    "    image_count += 1\n",
    "    print 'Classifying patches in image {}.'.format(image_count)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            # Get feature vector from image patch\n",
    "            patch = image[top_left_x:top_left_x+width,\n",
    "                          top_left_y:top_left_y+width, :]\n",
    "            color = calc_color_hist(patch)\n",
    "            color = color.flatten()\n",
    "            (hog, hog_bins, magnitude_hist, magnitude_bins,\n",
    "             max_magnitude) = compute_hog(patch)\n",
    "            feature = np.concatenate((color, hog, magnitude_hist),\n",
    "                                    axis=0)\n",
    "            # Classify image\n",
    "            predict = clf.predict(feature)[0] # 0 = nonroof, 1 = roof\n",
    "            probs = clf.predict_proba(feature)[0]\n",
    "            nonroof_prob = probs[0]\n",
    "            roof_prob = probs[1]\n",
    "            # Decide where to save image patch\n",
    "            if predict == 1:\n",
    "                out_fn = (roof_dir + os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '_' + str(roof_prob) +  '.png')\n",
    "            else:\n",
    "                out_fn = (nonroof_dir + os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '_' + str(nonroof_prob) + '.png')\n",
    "            # Save image patch\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y,\n",
    "                           width, height)\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)\n",
    "t1 = time.time()\n",
    "print 'Classified image patches for {} images in {} seconds.'.format(\n",
    "                                        image_count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this a couple more times to get more hard training examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Writing annotated classified images\n",
    "Our next step will be to save annotated versions of our classified images so that we can visually inspect them to see how well our classifier is doing. Some things that we want to do here are:\n",
    "\n",
    "- Draw bounding boxes around patches identified as roofs\n",
    "- Use non-maximum suppression to keep only the best patch out of overlapping patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Draw all bounding boxes around patches classified as roofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/samples/'\n",
    "roof_dir = '../images/forest/classify/roof/'\n",
    "nonroof_dir = '../images/forest/classify/nonroof/'\n",
    "annotated_dir = '../images/forest/classify/annotated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='auto', criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest classifier on all training examples\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying patches in image 1.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_patch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-89df90abf9aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m                           '_' + str(count) + '.png')\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Save image patch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             save_patch(image, out_fn, top_left_x, top_left_y,\n\u001b[0m\u001b[0;32m     57\u001b[0m                            width, height)\n\u001b[0;32m     58\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'save_patch' is not defined"
     ]
    }
   ],
   "source": [
    "# Classify each image patch\n",
    "width = 80\n",
    "height = 80\n",
    "image_count = 0\n",
    "font = cv2.FONT_HERSHEY_TRIPLEX\n",
    "annotation_color = (0,255,0) # green\n",
    "t0 = time.time()\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    # Load in image and one to annotate\n",
    "    image = cv2.imread(image_fn)\n",
    "    image_out = cv2.imread(image_fn)\n",
    "    image_count += 1\n",
    "    print 'Classifying patches in image {}.'.format(image_count)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            # Get feature vector from image patch\n",
    "            patch = image[top_left_x:top_left_x+height,\n",
    "                          top_left_y:top_left_y+width, :]\n",
    "            color = calc_color_hist(patch)\n",
    "            color = color.flatten()\n",
    "            #(hog, hog_bins, magnitude_hist, magnitude_bins,\n",
    "            # max_magnitude) = compute_hog(patch)\n",
    "            (hog, magnitude_hist) = compute_advanced_hog(patch)\n",
    "            feature = np.concatenate((color, hog, magnitude_hist),\n",
    "                                    axis=0)\n",
    "            # Classify image patch\n",
    "            predict = clf.predict(feature)[0] # 0 = nonroof, 1 = roof\n",
    "            probs = clf.predict_proba(feature)[0]\n",
    "            nonroof_prob = probs[0]\n",
    "            roof_prob = probs[1]\n",
    "            # Annotate image when patches classified as roofs\n",
    "            if predict == 1:\n",
    "                cv2.putText(image_out, str(roof_prob),\n",
    "                            (top_left_y + height/2, top_left_x + width/2),\n",
    "                            font, 0.5, annotation_color, thickness=1,\n",
    "                            lineType=cv2.CV_AA)\n",
    "                cv2.rectangle(image_out, (top_left_y, top_left_x),\n",
    "                             (top_left_y+height-1, top_left_x+width-1),\n",
    "                             annotation_color)\n",
    "            # Decide where to save image patch\n",
    "            if predict == 1:\n",
    "                out_fn = (roof_dir + str(roof_prob) + '_' +\n",
    "                          os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) +  '.png')\n",
    "            else:\n",
    "                out_fn = (nonroof_dir + str(nonroof_prob) + '_' +\n",
    "                          os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '.png')\n",
    "            # Save image patch\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y,\n",
    "                           width, height)\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)\n",
    "    # Save annotated image\n",
    "    image_out_fn = annotated_dir + os.path.basename(image_fn)\n",
    "    cv2.imwrite(image_out_fn, image_out)\n",
    "t1 = time.time()\n",
    "print 'Classified image patches for {} images in {} seconds.'.format(\n",
    "                                        image_count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adding non-maximum suppression\n",
    "What we should do here is save all the locations and probabilities of the patches classified as roofs within each image. Then at the end, we can choose the maximum probability patches.\n",
    "\n",
    "As a side note, I ran into an issue where Nautilus was no longer displaying preview images, which was really inconvenient for viewing my classification results. The following code deletes old previews so that new ones can be generated:\n",
    "\n",
    "`rm ~/.cache/thumbnails/ -R`\n",
    "\n",
    "Tested and works, thumbnails restored!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/classify/test/input/'\n",
    "roof_dir = '../images/forest/classify/roof/'\n",
    "nonroof_dir = '../images/forest/classify/nonroof/'\n",
    "annotated_dir = '../images/forest/classify/test/annotated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='auto', criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest classifier on all training examples\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "#clf1 = ensemble.RandomForestClassifier()\n",
    "#clf = ensemble.AdaBoostClassifier(base_estimator=clf1, n_estimators=50,\n",
    "#                                  random_state=0)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classifying patches in image 1.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must  match the input. Model n_features is 473 and  input n_features is 173 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2c012dd5d457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m                                     axis=0)\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# Classify image patch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mnonroof_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mroof_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    511\u001b[0m                              \u001b[0mbackend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"threading\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_parallel_helper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'predict_proba'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m             for e in self.estimators_)\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# Reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"all\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \"\"\"\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, args, kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_helper\u001b[1;34m(obj, methodname, *args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_parallel_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;34m\"\"\"Private helper to workaround Python 2 pickle limitations\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    588\u001b[0m                              \u001b[1;34m\" match the input. Model n_features is %s and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m                              \u001b[1;34m\" input n_features is %s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m                              % (self.n_features_, n_features))\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of features of the model must  match the input. Model n_features is 473 and  input n_features is 173 "
     ]
    }
   ],
   "source": [
    "# Classify each image patch\n",
    "width = 80\n",
    "height = 80\n",
    "image_count = 0\n",
    "font = cv2.FONT_HERSHEY_TRIPLEX\n",
    "annotation_color = (0,255,0) # green\n",
    "t0 = time.time()\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    # Load in image and one to annotate\n",
    "    image = cv2.imread(image_fn)\n",
    "    image_out = cv2.imread(image_fn)\n",
    "    image_count += 1\n",
    "    print 'Classifying patches in image {}.'.format(image_count)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    # Lists to store locations and probabilities for detected roofs\n",
    "    locations = []\n",
    "    roof_probabilities = []\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            # Get feature vector from image patch\n",
    "            patch = image[top_left_x:top_left_x+height,\n",
    "                          top_left_y:top_left_y+width, :]\n",
    "            color = calc_color_hist(patch)\n",
    "            color = color.flatten()\n",
    "            #(hog, hog_bins, magnitude_hist, magnitude_bins,\n",
    "            # max_magnitude) = compute_hog(patch)\n",
    "            (hog, magnitude_hist) = compute_advanced_hog(patch)\n",
    "            feature = np.concatenate((color, hog, magnitude_hist),\n",
    "                                    axis=0)\n",
    "            # Classify image patch\n",
    "            probs = clf.predict_proba(feature)[0]\n",
    "            nonroof_prob = probs[0]\n",
    "            roof_prob = probs[1]\n",
    "            # Classify with probability\n",
    "            #'''\n",
    "            if roof_prob > 0.6:\n",
    "                predict = 1\n",
    "            else:\n",
    "                predict = 0\n",
    "            #'''\n",
    "            # Classify with prediction\n",
    "            #predict = clf.predict(feature)[0] # 0 = nonroof, 1 = roof\n",
    "            # If classified as a roof, store location and probability\n",
    "            if predict == 1:\n",
    "                locations.append((top_left_x, top_left_y))\n",
    "                roof_probabilities.append(roof_prob)\n",
    "            # Decide where to save image patch\n",
    "            if predict == 1:\n",
    "                out_fn = (roof_dir + str(roof_prob) + '_' +\n",
    "                          os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) +  '.png')\n",
    "            else:\n",
    "                out_fn = (nonroof_dir + str(nonroof_prob) + '_' +\n",
    "                          os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '.png')\n",
    "            # Save image patch\n",
    "            '''\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y,\n",
    "                           width, height)\n",
    "            '''\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)\n",
    "    # Annotate image with non-maximum suppression\n",
    "    annotate_locs = []\n",
    "    annotate_probs = []\n",
    "    while locations:\n",
    "        max_prob = max(roof_probabilities)\n",
    "        max_ind = roof_probabilities.index(max_prob)\n",
    "        current_prob = roof_probabilities.pop(max_ind)\n",
    "        current_loc = locations.pop(max_ind)\n",
    "        # Check to see if overlapping\n",
    "        overlap = False\n",
    "        for i in range(len(annotate_locs)):\n",
    "            if ((abs(current_loc[0] - annotate_locs[i][0]) < width) and\n",
    "                (abs(current_loc[1] - annotate_locs[i][1]) < height)):\n",
    "                overlap = True\n",
    "        # If not overlapping, annotate and add to annotated list\n",
    "        if not overlap:\n",
    "            cv2.putText(image_out, str(current_prob),\n",
    "                        (current_loc[1] + height/2, current_loc[0] + width/2),\n",
    "                        font, 0.5, annotation_color, thickness=1,\n",
    "                        lineType=cv2.CV_AA)\n",
    "            cv2.rectangle(image_out, (current_loc[1], current_loc[0]),\n",
    "                         (current_loc[1]+height-1, current_loc[0]+width-1),\n",
    "                         annotation_color)\n",
    "            annotate_locs.append(current_loc)\n",
    "            annotate_probs.append(current_prob)\n",
    "    # Save annotated image\n",
    "    image_out_fn = annotated_dir + 'forest_' + os.path.basename(image_fn)[:-4] + '_boostthresh.jpg'\n",
    "    cv2.imwrite(image_out_fn, image_out)\n",
    "t1 = time.time()\n",
    "print 'Classified image patches for {} images in {} seconds.'.format(\n",
    "                                        image_count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Michael's get_images.py\n",
    "This is Michael Xie's code for downloading lots of images from the DHS locations. It is multi-threaded so it goes much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 37006 candidate locations\n",
      "37006 locations sampled\n",
      "Saving img_3.jpg\n",
      "Saving img_1.jpg\n",
      "Saving img_5.jpg\n",
      "Saving img_4.jpg\n",
      "Saving img_11.jpg\n",
      "Saving img_0.jpg\n",
      "Saving img_8.jpg\n",
      "Saving img_15.jpg\n",
      "Saving img_9.jpg\n",
      "Saving img_12.jpg\n",
      "Saving img_7.jpg\n",
      "Saving img_13.jpg\n",
      "Saving img_10.jpg\n",
      "Saving img_14.jpg\n",
      "Saving img_6.jpg\n",
      "Saving img_17.jpg\n",
      "Saving img_16.jpg\n",
      "Saving img_2.jpg\n",
      "Saving img_19.jpg\n",
      "Saving img_18.jpg\n",
      "Saving img_20.jpg\n",
      "Saving img_28.jpg\n",
      "Saving img_22.jpg\n",
      "Saving img_25.jpg\n",
      "Saving img_21.jpg\n",
      "Saving img_24.jpg\n",
      "Saving img_27.jpg\n",
      "Saving img_29.jpg\n",
      "Saving img_30.jpg\n",
      "Saving img_23.jpg\n",
      "Saving img_31.jpg\n",
      "Saving img_26.jpg\n",
      "Saving img_33.jpg\n",
      "Saving img_32.jpg\n",
      "Saving img_34.jpg\n",
      "Saving img_36.jpg\n",
      "Saving img_37.jpg\n",
      "Saving img_35.jpg\n",
      "Saving img_40.jpg\n",
      "Saving img_38.jpg\n",
      "Saving img_39.jpg\n",
      "Saving img_42.jpg\n",
      "Saving img_41.jpg\n",
      "Saving img_44.jpg\n",
      "Saving img_45.jpg\n",
      "Saving img_43.jpg\n",
      "Saving img_47.jpg\n",
      "Saving img_46.jpg\n",
      "Saving img_48.jpg\n",
      "Saving img_50.jpg\n",
      "Saving img_49.jpg\n",
      "Saving img_52.jpg\n",
      "Saving img_54.jpg\n",
      "Saving img_51.jpg\n",
      "Saving img_53.jpg\n",
      "Saving img_55.jpg\n",
      "Saving img_56.jpg\n",
      "Saving img_57.jpg\n",
      "Saving img_58.jpg\n",
      "Saving img_59.jpg\n",
      "Saving img_60.jpg\n",
      "Saving img_61.jpg\n",
      "Saving img_63.jpg\n",
      "Saving img_62.jpg\n",
      "Saving img_64.jpg\n",
      "Saving img_65.jpg\n",
      "Saving img_66.jpg\n",
      "Saving img_67.jpg\n",
      "Saving img_71.jpg\n",
      "Saving img_68.jpg\n",
      "Saving img_73.jpg\n",
      "Saving img_70.jpg\n",
      "Saving img_72.jpg\n",
      "Saving img_74.jpg\n",
      "Saving img_75.jpg\n",
      "Saving img_76.jpg\n",
      "Saving img_78.jpg\n",
      "Saving img_77.jpg\n",
      "Saving img_79.jpg\n",
      "Saving img_80.jpg\n",
      "Saving img_81.jpg\n",
      "Saving img_83.jpg\n",
      "Saving img_82.jpg\n",
      "Saving img_86.jpg\n",
      "Saving img_85.jpg\n",
      "Saving img_69.jpg\n",
      "Saving img_84.jpg\n",
      "Saving img_87.jpg\n",
      "Saving img_88.jpg\n",
      "Saving img_89.jpg\n",
      "Saving img_90.jpg\n",
      "SavingSaving  img_91.jpgimg_95.jpg\n",
      "\n",
      "Saving img_94.jpg\n",
      "Saving img_93.jpg\n",
      "Saving img_92.jpg\n",
      "Saving img_98.jpg\n",
      "Saving img_96.jpg\n",
      "Saving img_97.jpg\n",
      "Saving img_100.jpg\n",
      "Saving img_99.jpg\n",
      "Saving img_101.jpg\n",
      "Saving img_110.jpg\n",
      "Saving img_102.jpg\n",
      "Saving img_103.jpg\n",
      "Saving img_105.jpg\n",
      "Saving img_104.jpg\n",
      "Saving img_106.jpg\n",
      "Saving img_108.jpg\n",
      "Saving img_109.jpgSaving\n",
      " img_112.jpg\n",
      " Saving img_111.jpg\n",
      "Saving img_107.jpg\n",
      "Saving img_113.jpg\n",
      "Saving img_115.jpg\n",
      "Saving img_114.jpg\n",
      "Saving img_118.jpg\n",
      "Saving img_116.jpg\n",
      "Saving img_117.jpg\n",
      "Saving img_119.jpg\n",
      "Saving Savingimg_121.jpg \n",
      "img_120.jpg\n",
      "Saving img_123.jpg\n",
      "Saving img_122.jpg\n",
      "Saving img_124.jpg\n",
      "Saving img_127.jpg\n",
      "Saving img_128.jpg\n",
      "Saving img_132.jpg\n",
      "Saving img_126.jpg\n",
      "Saving img_130.jpg\n",
      "Saving img_125.jpg\n",
      "Saving img_131.jpg\n",
      "Saving img_133.jpg\n",
      "Saving img_129.jpg\n",
      "Saving img_134.jpg\n",
      "Saving img_135.jpg\n",
      "Saving img_136.jpg\n",
      "Saving img_137.jpg\n",
      "Saving img_139.jpg\n",
      "Saving img_145.jpg\n",
      "Saving img_147.jpg\n",
      "Saving img_140.jpg\n",
      "Saving img_138.jpg\n",
      "Saving img_141.jpg\n",
      "Saving img_144.jpg\n",
      "Saving img_142.jpg\n",
      "Saving img_143.jpg\n",
      "Saving img_146.jpg\n",
      "Saving img_153.jpg\n",
      "Saving img_148.jpg\n",
      "Saving img_149.jpg\n",
      "Saving img_151.jpg\n",
      "Saving img_152.jpg\n",
      "Saving img_163.jpg\n",
      "Saving img_150.jpg\n",
      "Saving img_155.jpg\n",
      "Saving img_154.jpg\n",
      "Saving img_158.jpg\n",
      "Saving img_160.jpg\n",
      "Saving img_157.jpg\n",
      "Saving img_159.jpg\n",
      "Saving img_156.jpg\n",
      "Saving img_161.jpg\n",
      "Saving img_164.jpg\n",
      " Saving img_162.jpg\n",
      "Saving img_166.jpg\n",
      "Saving img_165.jpg\n",
      "Saving img_171.jpg\n",
      "Saving img_167.jpg\n",
      "Saving img_168.jpg\n",
      "Saving img_170.jpg\n",
      "Saving img_169.jpg\n",
      "Saving img_172.jpg\n",
      "Saving img_173.jpg\n",
      "Saving img_176.jpg\n",
      "Saving img_175.jpg\n",
      "Saving img_174.jpg\n",
      "Saving img_177.jpg\n",
      "Saving img_178.jpg\n",
      "Saving img_179.jpg\n",
      "Saving img_180.jpg\n",
      "Saving img_181.jpg\n",
      "Saving img_183.jpg\n",
      "Saving img_182.jpg\n",
      "Saving img_184.jpg\n",
      "Saving img_185.jpg\n",
      "Saving img_186.jpg\n",
      "Saving img_188.jpg\n",
      "Saving img_187.jpg\n",
      "Saving img_189.jpg\n",
      "Saving img_190.jpg\n",
      "Saving img_191.jpgSaving\n",
      " img_194.jpg\n",
      "Saving img_192.jpg\n",
      "Saving img_195.jpg\n",
      "Saving img_196.jpg\n",
      "Saving img_197.jpg\n",
      "Saving img_193.jpg\n",
      "Saving img_199.jpg\n",
      "Saving img_198.jpg\n",
      "Saving img_201.jpg\n",
      "Saving img_204.jpg\n",
      "Saving img_202.jpg\n",
      "Saving img_203.jpg\n",
      "Saving img_200.jpg\n",
      "Saving img_205.jpg\n",
      "Saving img_206.jpg\n",
      "Saving img_208.jpg\n",
      "Saving img_207.jpg\n",
      "Saving img_215.jpg\n",
      "Saving img_210.jpg\n",
      "Saving img_209.jpg\n",
      "Saving img_214.jpg\n",
      "Saving img_216.jpg\n",
      "Saving img_212.jpg\n",
      "Saving img_211.jpg\n",
      "Saving img_213.jpg\n",
      "Saving img_218.jpg\n",
      "Saving img_219.jpg\n",
      "Saving img_217.jpg\n",
      "Saving img_221.jpg\n",
      "Saving img_220.jpg\n",
      "Saving img_223.jpg\n",
      "Saving img_225.jpg\n",
      "Saving img_224.jpg\n",
      "Saving img_226.jpg\n",
      "Saving img_227.jpg\n",
      "Saving img_229.jpg\n",
      "Saving img_222.jpg\n",
      "Saving img_230.jpg\n",
      "Saving img_228.jpg\n",
      "Saving img_231.jpg\n",
      "Saving img_233.jpg\n",
      "Saving img_234.jpg\n",
      "Saving img_235.jpg\n",
      "Saving img_242.jpg\n",
      "Saving img_237.jpg\n",
      "Saving img_236.jpg\n",
      "Saving img_238.jpg\n",
      "Saving img_240.jpg\n",
      "Saving img_241.jpg\n",
      "Saving img_239.jpg\n",
      "Saving img_232.jpg\n",
      "Saving img_245.jpg\n",
      "Saving img_244.jpg\n",
      "Saving img_246.jpg\n",
      "Saving img_243.jpg\n",
      "Saving img_247.jpg\n",
      "Saving img_249.jpg\n",
      "Saving img_248.jpg\n",
      "Saving img_250.jpg\n",
      "Saving img_252.jpg\n",
      "Saving img_253.jpg\n",
      "Saving img_254.jpg\n",
      "Saving img_251.jpg\n",
      "Saving img_261.jpg\n",
      "Saving img_255.jpg\n",
      "Saving img_256.jpg\n",
      "Saving img_257.jpg\n",
      "Saving img_258.jpg\n",
      "Saving img_260.jpg\n",
      "Saving img_259.jpg\n",
      "Saving img_262.jpg\n",
      "Saving img_264.jpg\n",
      "Saving img_263.jpg\n",
      "Saving img_265.jpg\n",
      "Saving img_266.jpg\n",
      "Saving img_267.jpg\n",
      "Saving img_269.jpg\n",
      "Saving img_268.jpg\n",
      "Saving img_271.jpg\n",
      "Saving img_277.jpg\n",
      "Saving img_272.jpg\n",
      "Saving img_270.jpg\n",
      "Saving img_273.jpg\n",
      "Saving img_274.jpg\n",
      "Saving img_275.jpg\n",
      "Saving img_276.jpg\n",
      "Saving img_278.jpg\n",
      "Saving img_279.jpg\n",
      "Saving img_280.jpg\n",
      "Saving img_281.jpg\n",
      "Saving img_282.jpg\n",
      "Saving img_284.jpg\n",
      "Saving img_283.jpg\n",
      "Saving img_287.jpg\n",
      "Saving img_285.jpg\n",
      "Saving img_286.jpg\n",
      "Saving img_289.jpg\n",
      "Saving img_291.jpg\n",
      "Saving img_290.jpg\n",
      "Saving img_292.jpg\n",
      "Saving img_294.jpg\n",
      "Saving img_293.jpg\n",
      "Saving img_295.jpg\n",
      "Saving img_296.jpg\n",
      "Saving img_297.jpg\n",
      "Saving img_288.jpg\n",
      "Saving img_298.jpg\n",
      "Saving img_299.jpg\n",
      "Saving img_300.jpg\n",
      "Saving img_308.jpg\n",
      "Saving img_303.jpg\n",
      "Saving img_304.jpg\n",
      "Saving img_302.jpg\n",
      "Saving img_305.jpg\n",
      "Saving img_309.jpg\n",
      "Saving img_306.jpgSaving\n",
      " img_301.jpg\n",
      "Saving img_307.jpg\n",
      "Saving img_310.jpg\n",
      "Saving img_312.jpg\n",
      "Saving img_313.jpg\n",
      "Saving img_314.jpg\n",
      "Saving img_316.jpg\n",
      "Saving img_315.jpg\n",
      "Saving img_318.jpg\n",
      "Saving img_319.jpg\n",
      "Saving img_321.jpg\n",
      "Saving img_320.jpg\n",
      "Saving img_322.jpg\n",
      "Saving img_324.jpg\n",
      "Saving img_329.jpg\n",
      "Saving img_325.jpg\n",
      "Saving img_323.jpg\n",
      "Saving img_326.jpg\n",
      "Saving img_327.jpg\n",
      "Saving img_328.jpg\n",
      "Saving img_330.jpg\n",
      "Saving img_331.jpg\n",
      "Saving img_311.jpg\n",
      "Saving img_333.jpg\n",
      "Saving img_332.jpg\n",
      "Saving img_334.jpg\n",
      "Saving img_337.jpg\n",
      "Saving img_336.jpg\n",
      "Saving img_339.jpg\n",
      "Saving img_335.jpg\n",
      "Saving img_341.jpg\n",
      "Saving img_342.jpg\n",
      "Saving img_338.jpg\n",
      "Saving img_345.jpg\n",
      "Saving img_344.jpg\n",
      "Saving img_346.jpg\n",
      "Saving img_348.jpg\n",
      "Saving img_343.jpg\n",
      "Saving img_349.jpg\n",
      "Saving img_347.jpg\n",
      "Saving img_350.jpg\n",
      "Saving img_351.jpg\n",
      "Saving img_357.jpg\n",
      "Saving img_352.jpg\n",
      "Saving img_340.jpg\n",
      "Saving img_353.jpg\n",
      "Saving img_354.jpg\n",
      "Saving img_355.jpg\n",
      "Saving img_356.jpg\n",
      "Saving img_358.jpg\n",
      "Saving img_359.jpg\n",
      "Saving img_361.jpg\n",
      "Saving img_360.jpg\n",
      "Saving img_363.jpg\n",
      "Saving img_365.jpg\n",
      "Saving img_364.jpg\n",
      "Saving img_362.jpg\n",
      "Saving img_367.jpg\n",
      "Saving img_366.jpg\n",
      "Saving img_368.jpg\n",
      "Saving img_369.jpg\n",
      "Saving img_370.jpg\n",
      "Saving img_371.jpg\n",
      "Saving img_378.jpg\n",
      "Saving img_374.jpg\n",
      "Saving img_372.jpg\n",
      "Saving img_373.jpg\n",
      "Saving img_375.jpg\n",
      "Saving img_376.jpg\n",
      "Saving img_377.jpg\n",
      "Saving img_379.jpg\n",
      "Saving img_382.jpg\n",
      "Saving img_384.jpg\n",
      "Saving img_386.jpg\n",
      "Saving img_380.jpg\n",
      " Saving img_385.jpg\n",
      "Saving img_383.jpg\n",
      "Saving img_387.jpg\n",
      "Saving img_388.jpg\n",
      "Saving img_390.jpg\n",
      "Saving img_381.jpg\n",
      "Saving img_389.jpg\n",
      "Saving img_391.jpg\n",
      "Saving img_392.jpg\n",
      "Saving img_397.jpg\n",
      "Saving img_394.jpg\n",
      "Saving img_393.jpg\n",
      "Saving img_395.jpg\n",
      "Saving img_396.jpg\n",
      "Saving img_398.jpg\n",
      "Saving img_399.jpg\n",
      "Saving img_400.jpg\n",
      "Saving img_402.jpg\n",
      "Saving img_404.jpg\n",
      "Saving img_401.jpg\n",
      "Saving img_405.jpg\n",
      "Saving img_406.jpg\n",
      "Saving img_407.jpg\n",
      "Saving img_403.jpg\n",
      "Saving img_409.jpg\n",
      "Saving img_408.jpg\n",
      "Saving img_410.jpg\n",
      "Saving img_411.jpg\n",
      "Saving img_413.jpg\n",
      "Saving img_412.jpg\n",
      "Saving img_415.jpg\n",
      "Saving img_414.jpg\n",
      "Saving img_417.jpg\n",
      "Saving img_418.jpg\n",
      "Saving img_416.jpg\n",
      "Saving img_419.jpg\n",
      "Saving img_420.jpg\n",
      "Saving img_421.jpg\n",
      "Saving img_422.jpg\n",
      "Saving img_424.jpg\n",
      "Saving img_426.jpg\n",
      "Saving img_427.jpg\n",
      "Saving img_425.jpg\n",
      "Saving img_428.jpg\n",
      "Saving img_429.jpg\n",
      "Saving img_430.jpg\n",
      "Saving img_437.jpg\n",
      "Saving img_433.jpg\n",
      "Saving img_431.jpg\n",
      "Saving img_432.jpg\n",
      "Saving img_434.jpg\n",
      "Saving img_436.jpg\n",
      "Saving img_438.jpg\n",
      "Saving img_435.jpg\n",
      "Saving img_441.jpg\n",
      "Saving img_439.jpg\n",
      "Saving img_423.jpg\n",
      "Saving img_440.jpg\n",
      "Saving img_442.jpg\n",
      "Saving img_443.jpg\n",
      "Saving img_446.jpg\n",
      "Saving img_445.jpg\n",
      "Saving img_447.jpg\n",
      "Saving img_444.jpg\n",
      "Saving img_448.jpg\n",
      "Saving img_449.jpg\n",
      "Saving img_450.jpg\n",
      "Saving img_451.jpg\n",
      "Saving img_454.jpg\n",
      "Saving img_452.jpg\n",
      "Saving img_453.jpg\n",
      "Saving img_460.jpg\n",
      "Saving img_457.jpg\n",
      "Saving img_455.jpg\n",
      "Saving img_458.jpg\n",
      "Saving img_456.jpg\n",
      "Saving img_459.jpg\n",
      "Saving img_465.jpg\n",
      "Saving img_462.jpg\n",
      "Saving img_464.jpg\n",
      "Saving img_461.jpg\n",
      "Saving img_466.jpg\n",
      "Saving img_468.jpg\n",
      "Saving img_469.jpg\n",
      "Saving img_470.jpg\n",
      "Saving img_471.jpg\n",
      "Saving img_473.jpg\n",
      "Saving img_472.jpg\n",
      "Saving img_474.jpg\n",
      "Saving img_475.jpg\n",
      "Saving img_477.jpg\n",
      "Saving img_476.jpg\n",
      "Saving img_463.jpg\n",
      "Saving img_467.jpg\n",
      "Saving img_478.jpg\n",
      "Saving img_479.jpg\n",
      "Saving img_480.jpg\n",
      "Saving img_481.jpg\n",
      "Saving img_482.jpg\n",
      "Saving img_483.jpg\n",
      "Saving img_486.jpg\n",
      "Saving img_488.jpg\n",
      "Saving img_485.jpg\n",
      "Saving img_484.jpg\n",
      "Saving img_487.jpg\n",
      "Saving img_489.jpg\n",
      "Saving img_492.jpg\n",
      "Saving img_490.jpg\n",
      "Saving img_491.jpg\n",
      "Saving img_495.jpg\n",
      "Saving img_494.jpg\n",
      "Saving img_493.jpg\n",
      "Saving img_496.jpg\n",
      "Saving img_497.jpg\n",
      "Saving img_498.jpg\n",
      "Saving img_499.jpg\n",
      "Saving img_500.jpg\n",
      "Saving img_503.jpg\n",
      "Saving img_501.jpg\n",
      "Saving img_502.jpg\n",
      "Saving img_504.jpg\n",
      "Saving img_506.jpg\n",
      "Saving img_505.jpg\n",
      "Saving img_510.jpg\n",
      "Saving img_507.jpg\n",
      "Saving img_515.jpg\n",
      "Saving img_509.jpg\n",
      "Saving img_508.jpg\n",
      "Saving img_512.jpg\n",
      "Saving img_514.jpg\n",
      "Saving img_513.jpg\n",
      "Saving img_511.jpg\n",
      "Saving img_516.jpg\n",
      "Saving img_524.jpg\n",
      "Saving img_517.jpg\n",
      "Saving img_519.jpg\n",
      "Saving img_529.jpg\n",
      "Saving img_521.jpg\n",
      "Saving img_518.jpg\n",
      "Saving img_520.jpg\n",
      "Saving img_522.jpg\n",
      "Saving img_525.jpg\n",
      "Saving img_523.jpg\n",
      "Saving img_528.jpg\n",
      "Saving img_526.jpg\n",
      "Saving img_527.jpg\n",
      "Saving img_530.jpg\n",
      "Saving img_534.jpg\n",
      "Saving img_531.jpg\n",
      "Saving img_533.jpg\n",
      "Saving img_532.jpg\n",
      "Saving img_538.jpg\n",
      "Saving img_535.jpg\n",
      "Saving img_537.jpg\n",
      "Saving img_536.jpg\n",
      "Saving img_540.jpg\n",
      "Saving img_547.jpg\n",
      "Saving img_539.jpg\n",
      "Saving img_541.jpg\n",
      "Saving img_543.jpg\n",
      "Saving img_542.jpg\n",
      "Saving img_544.jpg\n",
      "Saving img_546.jpg\n",
      "Saving img_545.jpg\n",
      "Saving img_548.jpg\n",
      "Saving img_551.jpg\n",
      "Saving img_549.jpg\n",
      "Saving img_550.jpg\n",
      "Saving img_552.jpg\n",
      "Saving img_554.jpg\n",
      "Saving img_553.jpg\n",
      "Saving img_556.jpg\n",
      "Saving img_555.jpg\n",
      "Saving img_558.jpg\n",
      "Saving img_557.jpg\n",
      "Saving img_561.jpg\n",
      "Saving img_559.jpg\n",
      "Saving img_560.jpg\n",
      "Saving img_562.jpg\n",
      "Saving img_563.jpg\n",
      "Saving img_564.jpg\n",
      "Saving img_565.jpg\n",
      "Saving img_566.jpg\n",
      "Saving img_567.jpg\n",
      "Saving img_570.jpg\n",
      "Saving img_569.jpg\n",
      "Saving img_568.jpg\n",
      "Saving img_573.jpg\n",
      "Saving img_571.jpg\n",
      "Saving img_572.jpg\n",
      "Saving img_577.jpg\n",
      "Saving img_576.jpg\n",
      "Saving img_578.jpg\n",
      "Saving img_575.jpg\n",
      "Saving img_580.jpg\n",
      "Saving img_574.jpg\n",
      "Saving img_579.jpg\n",
      "Saving img_581.jpg\n",
      "Saving img_584.jpg\n",
      "Saving img_583.jpg\n",
      "Saving img_582.jpg\n",
      "Saving img_585.jpg\n",
      "Saving img_586.jpg\n",
      "Saving img_587.jpg\n",
      "Saving img_589.jpg\n",
      "Saving img_590.jpg\n",
      " SavingSaving  img_592.jpgimg_588.jpg\n",
      "\n",
      "Saving img_594.jpg\n",
      "Saving img_591.jpg\n",
      "Saving img_595.jpg\n",
      "Saving img_596.jpg\n",
      "Saving img_593.jpg\n",
      "Saving img_597.jpg\n",
      "Saving img_598.jpg\n",
      "Saving img_599.jpg\n",
      "Saving img_601.jpg\n",
      "Saving img_600.jpg\n",
      "Saving img_603.jpg\n",
      "Saving img_602.jpg\n",
      "Saving img_605.jpg\n",
      "Saving img_604.jpg\n",
      "Saving img_607.jpg\n",
      "Saving img_609.jpg\n",
      "Saving img_606.jpg\n",
      "Saving img_613.jpg\n",
      "Saving img_608.jpg\n",
      "Saving img_610.jpg\n",
      "Saving img_612.jpg\n",
      "Saving img_611.jpg\n",
      " Saving img_614.jpg\n",
      "Saving img_615.jpg\n",
      "Saving img_616.jpg\n",
      "Saving img_617.jpg\n",
      "Saving img_621.jpg\n",
      "Saving img_618.jpg\n",
      "Saving img_619.jpg\n",
      "Saving img_620.jpg\n",
      "Saving img_622.jpg\n",
      "Saving img_623.jpg\n",
      "Saving img_627.jpg\n",
      "Saving img_624.jpg\n",
      "Saving img_625.jpg\n",
      "Saving img_626.jpg\n",
      "Saving img_628.jpg\n",
      "Saving img_629.jpg\n",
      "Saving img_631.jpg\n",
      "Saving img_630.jpg\n",
      "Saving img_633.jpg\n",
      "Saving img_632.jpg\n",
      "Saving img_635.jpg\n",
      "Saving img_636.jpg\n",
      "Saving img_634.jpg\n",
      "Saving img_637.jpg\n",
      "Saving img_645.jpg\n",
      "Saving img_638.jpg\n",
      "Saving img_640.jpg\n",
      "Saving img_641.jpg\n",
      "Saving img_642.jpg\n",
      "Saving img_644.jpg\n",
      "Saving img_643.jpg\n",
      "Saving img_646.jpg\n",
      "Saving img_647.jpg\n",
      "Saving img_639.jpg\n",
      "Saving img_648.jpg\n",
      "Saving img_649.jpg\n",
      "Saving img_653.jpg\n",
      "Saving img_650.jpg\n",
      "Saving img_652.jpg\n",
      "Saving img_651.jpg\n",
      "Saving img_655.jpg\n",
      "Saving img_654.jpg\n",
      "Saving img_656.jpg\n",
      "Saving img_659.jpg\n",
      "Saving img_657.jpg\n",
      "Saving img_658.jpg\n",
      "Saving img_662.jpg\n",
      "Saving img_661.jpg\n",
      "Saving img_660.jpg\n",
      "Saving img_664.jpg\n",
      "Saving img_665.jpg\n",
      "Saving img_663.jpg\n",
      "Saving img_667.jpg\n",
      "Saving img_666.jpg\n",
      "Saving img_668.jpg\n",
      "Saving img_669.jpg\n",
      "Saving img_671.jpg\n",
      "Saving img_670.jpg\n",
      "Saving img_672.jpg\n",
      "Saving img_680.jpg\n",
      "Saving img_674.jpg\n",
      "Saving img_673.jpg\n",
      "Saving img_675.jpg\n",
      "Saving img_676.jpg\n",
      "Saving img_677.jpg\n",
      "Saving img_682.jpg\n",
      "Saving img_678.jpg\n",
      "Saving img_681.jpg\n",
      "Saving img_679.jpg\n",
      "Saving img_683.jpg\n",
      "Saving img_686.jpg\n",
      "Saving img_685.jpg\n",
      "Saving img_684.jpg\n",
      "Saving img_687.jpg\n",
      "Saving img_689.jpg\n",
      "Saving img_688.jpg\n",
      "Saving img_691.jpg\n",
      "Saving img_692.jpg\n",
      "Saving img_690.jpg\n",
      "Saving img_694.jpg\n",
      "Saving img_696.jpg\n",
      "Saving img_693.jpg\n",
      "Saving img_699.jpg\n",
      "Saving img_695.jpg\n",
      "Saving img_697.jpg\n",
      "Saving img_698.jpg\n",
      "Saving img_701.jpg\n",
      "Saving img_706.jpg\n",
      "Saving img_700.jpg\n",
      "Saving img_702.jpg\n",
      "Saving img_703.jpg\n",
      "Saving img_704.jpg\n",
      "Saving img_705.jpg\n",
      "Saving img_712.jpg\n",
      "Saving img_707.jpg\n",
      "Saving img_710.jpg\n",
      "Saving img_708.jpg\n",
      "Saving img_709.jpg\n",
      "Saving img_711.jpg\n",
      "Saving img_713.jpg\n",
      "Saving img_714.jpg\n",
      "Saving img_715.jpg\n",
      "Saving img_717.jpg\n",
      "Saving img_716.jpg\n",
      "Saving img_722.jpg\n",
      "Saving img_720.jpg\n",
      "Saving img_718.jpg\n",
      "Saving img_719.jpg\n",
      "Saving img_721.jpg\n",
      "Saving img_723.jpg\n",
      "Saving img_724.jpg\n",
      "Saving img_725.jpg\n",
      "Saving img_727.jpg\n",
      "Saving img_729.jpg\n",
      "Saving img_728.jpg\n",
      "Saving img_726.jpg\n",
      "Saving img_730.jpg\n",
      "Saving img_732.jpg\n",
      "Saving img_734.jpg\n",
      "Saving img_735.jpg\n",
      "Saving img_737.jpg\n",
      "Saving img_731.jpg\n",
      "Saving img_738.jpg\n",
      "Saving img_733.jpg\n",
      "Saving img_736.jpg\n",
      "Saving img_740.jpg\n",
      " Saving img_739.jpg\n",
      "Saving img_741.jpg\n",
      "Saving img_743.jpg\n",
      "Saving img_745.jpg\n",
      "Saving img_747.jpg\n",
      "Saving img_744.jpg\n",
      "Saving img_746.jpg\n",
      "Saving img_748.jpg\n",
      "Saving img_751.jpg\n",
      "Saving img_750.jpg\n",
      "Saving img_749.jpg\n",
      "Saving img_753.jpg\n",
      "Saving img_752.jpg\n",
      "Saving img_754.jpg\n",
      "Saving img_756.jpg\n",
      "Saving img_757.jpg\n",
      "Saving img_755.jpg\n",
      "Saving img_758.jpg\n",
      "Saving img_759.jpg\n",
      "Saving img_761.jpg\n",
      "Saving img_760.jpg\n",
      "Saving img_763.jpg\n",
      "Saving img_742.jpg\n",
      "Saving img_766.jpg\n",
      "Saving img_765.jpg\n",
      "Saving img_767.jpg\n",
      "Saving img_764.jpg\n",
      "Saving img_770.jpg\n",
      "Saving img_762.jpg\n",
      "Saving img_769.jpg\n",
      "Saving img_772.jpg\n",
      "Saving img_768.jpg\n",
      "Saving img_771.jpg\n",
      "Saving img_775.jpg\n",
      "Saving img_773.jpg\n",
      "Saving img_774.jpg\n",
      "Saving img_777.jpg\n",
      "Saving img_778.jpg\n",
      "Saving img_776.jpg\n",
      "Saving img_785.jpg\n",
      "Saving img_779.jpg\n",
      "Saving img_782.jpg\n",
      "Saving img_781.jpg\n",
      "Saving img_783.jpg\n",
      "Saving img_784.jpg\n",
      "Saving img_786.jpg\n",
      "Saving img_788.jpg\n",
      "Saving img_780.jpg\n",
      "Saving img_790.jpg\n",
      "Saving img_787.jpg\n",
      "Saving img_791.jpg\n",
      "Saving img_789.jpg\n",
      "Saving img_794.jpg\n",
      "Saving img_792.jpg\n",
      "Saving img_795.jpg\n",
      "Saving img_796.jpg\n",
      "Saving img_797.jpg\n",
      "Saving img_798.jpg\n",
      "Saving img_799.jpg\n",
      "Saving img_802.jpg\n",
      "Saving img_801.jpg\n",
      "Saving img_800.jpg\n",
      "Saving img_803.jpg\n",
      "Saving img_804.jpg\n",
      "Saving img_806.jpg\n",
      "Saving img_793.jpg\n",
      "Saving img_805.jpg\n",
      "Saving img_809.jpg\n",
      "Saving img_807.jpg\n",
      "Saving img_808.jpg\n",
      "Saving img_810.jpg\n",
      "Saving img_816.jpg\n",
      "Saving img_818.jpg\n",
      "Saving img_811.jpg\n",
      "Saving img_813.jpg\n",
      "Saving img_814.jpg\n",
      "Saving img_815.jpg\n",
      "Saving img_812.jpg\n",
      "Saving img_817.jpg\n",
      "Saving img_819.jpg\n",
      "Saving img_820.jpg\n",
      "Saving img_822.jpg\n",
      "Saving img_821.jpg\n",
      "Saving img_829.jpg\n",
      "Saving img_823.jpg\n",
      "Saving img_827.jpg\n",
      "Saving img_824.jpgSaving img_826.jpg\n",
      "\n",
      "Saving img_825.jpg\n",
      "Saving img_828.jpg\n",
      "Saving img_831.jpg\n",
      "Saving img_830.jpg\n",
      "Saving img_832.jpg\n",
      "Saving img_833.jpg\n",
      "Saving img_835.jpg\n",
      "Saving img_836.jpg\n",
      "Saving img_837.jpg\n",
      "Saving img_834.jpg\n",
      "Saving img_840.jpg\n",
      "Saving img_839.jpg\n",
      "Saving img_843.jpg\n",
      "Saving img_838.jpg\n",
      "Saving img_844.jpg\n",
      "Saving img_841.jpg\n",
      "Saving img_842.jpg\n",
      "Saving img_846.jpg\n",
      "Saving img_851.jpg\n",
      "Saving img_848.jpg\n",
      "Saving img_847.jpg\n",
      "Saving img_845.jpg\n",
      "Saving img_849.jpg\n",
      "Saving img_852.jpg\n",
      "Saving img_853.jpg\n",
      "Saving img_850.jpg\n",
      "Saving img_854.jpg\n",
      "Saving img_855.jpg\n",
      "Saving img_859.jpg\n",
      "Saving img_856.jpg\n",
      "Saving img_857.jpg\n",
      "Saving img_860.jpg\n",
      "Saving img_863.jpg\n",
      "Saving img_858.jpg\n",
      "Saving img_861.jpg\n",
      "Saving img_862.jpg\n",
      "Saving img_869.jpg\n",
      "Saving img_865.jpg\n",
      "Saving img_864.jpg\n",
      "Saving img_866.jpg\n",
      "Saving img_867.jpg\n",
      "Saving img_868.jpg\n",
      "Saving img_871.jpg\n",
      "Saving img_873.jpgSaving img_872.jpg\n",
      "\n",
      "Saving img_870.jpg\n",
      "Saving img_875.jpg\n",
      "Saving img_876.jpgSaving\n",
      " img_874.jpg\n",
      "Saving img_877.jpg\n",
      "Saving img_880.jpg\n",
      "Saving img_882.jpg\n",
      "Saving img_883.jpg\n",
      "Saving img_881.jpg\n",
      "Saving img_884.jpg\n",
      "Saving img_886.jpg\n",
      "Saving img_885.jpg\n",
      "Saving img_878.jpg\n",
      "Saving img_887.jpg\n",
      "Saving img_890.jpg\n",
      "Saving img_888.jpg\n",
      "Saving img_889.jpg\n",
      "Saving img_893.jpg\n",
      "Saving img_891.jpg\n",
      "Saving img_892.jpg\n",
      "Saving img_879.jpg\n",
      "Saving img_894.jpg\n",
      "Saving img_895.jpg\n",
      "Saving img_896.jpg\n",
      "Saving img_897.jpg\n",
      "Saving img_898.jpg\n",
      "Saving img_899.jpg\n",
      "Saving img_900.jpg\n",
      "Saving img_901.jpg\n",
      "Saving img_903.jpg\n",
      "Saving img_905.jpg\n",
      "Saving img_907.jpg\n",
      "Saving img_904.jpg\n",
      "Saving img_906.jpg\n",
      "Saving img_909.jpg\n",
      "Saving img_912.jpg\n",
      "Saving img_911.jpg\n",
      "Saving img_910.jpg\n",
      "Saving img_913.jpg\n",
      "Saving img_914.jpg\n",
      "Saving img_902.jpg\n",
      "Saving img_908.jpg\n",
      "Saving img_916.jpg\n",
      "Saving img_915.jpg\n",
      "Saving img_920.jpg\n",
      "Saving img_918.jpg\n",
      "Saving img_917.jpg\n",
      " Saving img_922.jpg\n",
      "Saving img_921.jpg\n",
      "Saving img_923.jpg\n",
      "Saving img_929.jpgSaving\n",
      " img_924.jpg\n",
      "Saving img_926.jpg\n",
      "Saving img_927.jpg\n",
      "Saving img_931.jpg\n",
      "Saving img_925.jpg\n",
      "Saving img_928.jpg\n",
      "Saving img_930.jpg\n",
      "Saving img_932.jpg\n",
      "Saving img_934.jpg\n",
      "Saving img_933.jpgSaving\n",
      " img_935.jpg\n",
      "Saving img_919.jpg\n",
      "Saving img_937.jpg\n",
      "Saving img_936.jpg\n",
      "Saving img_939.jpg\n",
      "Saving img_941.jpg\n",
      "Saving img_943.jpg\n",
      "Saving img_942.jpg\n",
      "Saving img_944.jpg\n",
      "Saving img_940.jpg\n",
      "Saving img_945.jpg\n",
      "Saving img_946.jpg\n",
      "Saving img_952.jpg\n",
      "Saving img_938.jpg\n",
      "Saving img_951.jpg\n",
      "Saving img_949.jpg\n",
      "Saving img_948.jpg\n",
      "Saving img_950.jpg\n",
      "Saving img_953.jpg\n",
      "Saving img_954.jpg\n",
      "Saving img_955.jpg\n",
      "Saving img_956.jpg\n",
      "Saving img_959.jpg\n",
      "Saving img_958.jpg\n",
      "Saving img_957.jpg\n",
      "Saving img_947.jpg\n",
      "Saving img_960.jpg\n",
      "Saving img_961.jpg\n",
      "Saving img_963.jpg\n",
      "Saving img_965.jpg\n",
      "Saving img_964.jpg\n",
      "Saving img_974.jpg\n",
      "Saving img_966.jpg\n",
      "Saving img_967.jpg\n",
      "Saving img_970.jpg\n",
      "Saving img_968.jpg\n",
      "Saving img_962.jpg\n",
      "Saving img_969.jpg\n",
      "Saving img_972.jpg\n",
      "Saving img_971.jpg\n",
      "Saving img_973.jpg\n",
      "Saving img_975.jpg\n",
      "Saving img_976.jpg\n",
      "Saving img_977.jpg\n",
      "Saving img_978.jpg\n",
      "Saving img_980.jpg\n",
      "Saving img_985.jpg\n",
      "Saving img_981.jpg\n",
      "Saving img_984.jpg\n",
      "Saving img_979.jpg\n",
      "Saving img_986.jpg\n",
      "Saving img_983.jpg\n",
      "Saving img_987.jpg\n",
      "Saving img_982.jpg\n",
      "Saving img_988.jpg\n",
      "Saving img_989.jpg\n",
      "Saving img_990.jpg\n",
      "Saving img_992.jpg\n",
      "Saving img_991.jpg\n",
      "Saving img_993.jpg\n",
      "Saving img_995.jpg\n",
      "Saving img_994.jpg\n",
      "Saving img_996.jpg\n",
      "Saving img_998.jpg\n",
      "Saving img_997.jpg\n",
      "Saving img_999.jpg\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: '/afs/cs.stanford.edu/u/nealjean/scratch/GitHub/BuildingDetector/images/forest/samples/img_317.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/scratch/GitHub/poverty/nightlights_cnn/get_images.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                 \u001b[0mfile_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfile_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/afs/cs.stanford.edu/u/nealjean/scratch/GitHub/BuildingDetector/images/forest/samples/img_317.jpg'"
     ]
    }
   ],
   "source": [
    "%run /afs/cs.stanford.edu/u/nealjean/scratch/GitHub/poverty/nightlights_cnn/get_images.py --ol ~/scratch/GitHub/BuildingDetector/images/forest/image_data.txt \\\n",
    "-d ~/scratch/GitHub/BuildingDetector/images/forest/samples/ \\\n",
    "-m 1000 -s 400 -t 16 -z 19 -f /afs/cs.stanford.edu/u/nealjean/scratch/GitHub/poverty/nightlights_cnn/DHS_data/dhs_latlon.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Improving on HOG features\n",
    "At this point, the classifier does pretty well at classifying image patches into roof and nonroof classes. However, it doesn't take into account spatial information within the image patch, since the color histograms, basic HOG, and gradient magnitudes are all calculated over the whole image patch. It would be cool if we added extra HOG vectors for different parts of the image.\n",
    "\n",
    "Since each of our image patches is an 80x80 pixel square, it probably makes sense to split it into 16 20x20 pixel squares. Let's try to extract those features in addition to the ones that we already have and see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_advanced_hog(image_patch, normalize=True):\n",
    "    \"\"\"\n",
    "    This function takes an image patch as input and returns full-patch\n",
    "    and partial-patch HOG and gradient magnitude vectors.\n",
    "    :param image_patch: The image patch on which to compute the features\n",
    "    :param normalize: Normalizes the histograms before returning them\n",
    "    :returns: Full HOG and gradient magnitude vectors\n",
    "    \"\"\"\n",
    "    # Getting size of image patch\n",
    "    height = image_patch.shape[0]\n",
    "    width = image_patch.shape[1]\n",
    "\n",
    "    # Compute full-patch HOG and gradient magnitudes\n",
    "    (hog, hog_bins, magnitude_hist, magnitude_bins, max_magnitude) = \\\n",
    "                        compute_hog(image_patch)\n",
    "    \n",
    "    # Split patch into 16 equal size patches, add features from each\n",
    "    h_patch = height / 4\n",
    "    w_patch = width / 4\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            patch_ij = image_patch[i*h_patch:(i+1)*h_patch,\n",
    "                                   j*w_patch:(j+1)*w_patch,:]\n",
    "            # Compute sub-patch features\n",
    "            (hog_ij, hog_bins_ij, magnitude_hist_ij, magnitude_bins_ij,\n",
    "             max_magnitude_ij) = compute_hog(patch_ij)\n",
    "            # Concatenate with full-patch features\n",
    "            hog = np.concatenate((hog, hog_ij), axis=1)\n",
    "            magnitude_hist = np.concatenate((magnitude_hist,\n",
    "                                            magnitude_hist_ij), axis=1)\n",
    "    # Replace nan values with 0\n",
    "    hog = np.nan_to_num(hog)\n",
    "    magnitude_hist = np.nan_to_num(magnitude_hist)\n",
    "    \n",
    "    return (hog, magnitude_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fn = '../images/forest/training/nonroof/976.png'\n",
    "test = cv2.imread(test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[254]\n"
     ]
    }
   ],
   "source": [
    "print np.unique(test[40:60,0:20,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153,)\n",
      "(272,)\n"
     ]
    }
   ],
   "source": [
    "(hog, mag) = compute_advanced_hog(test)\n",
    "print hog.shape\n",
    "print mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nice, now let's use this new feature extractor to get more detailed HOG and gradient magnitude features and then see how well our classifier does using those richer feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Replacing custom HOG with skimage\n",
    "Before I had been using my own custom HOG features, which also included a histogram of gradient magnitudes. However, this was running very slow (~7 seconds to process each image), so I'm going to try replacing it with the `scikit-image` HOG implementation which runs much faster (up to 80x speedup).\n",
    "\n",
    "In order to do this, I'll need to rewrite some of my code to extract features from all training examples and store them, then import all of the training data. Hopefully the classifier works as well as it did before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images for roof class.\n",
      "Processed 200 images for roof class.\n",
      "Processed 300 images for roof class.\n",
      "Processed 400 images for roof class.\n",
      "Processed 500 images for roof class.\n",
      "Processed 600 images for roof class.\n",
      "Processed 700 images for roof class.\n",
      "Processed 800 images for roof class.\n",
      "Processed 900 images for roof class.\n",
      "Processed 1000 images for roof class.\n",
      "Processed 1100 images for roof class.\n",
      "Processed 1200 images for roof class.\n",
      "Processed 1300 images for roof class.\n",
      "Processed 1400 images for roof class.\n",
      "Processed 1500 images for roof class.\n",
      "Processed 1600 images for roof class.\n",
      "Processed 1700 images for roof class.\n",
      "Processed 1800 images for roof class.\n",
      "Processed 1900 images for roof class.\n",
      "Processed 2000 images for roof class.\n",
      "Processed 2100 images for roof class.\n",
      "Processed 2200 images for roof class.\n",
      "Processed 2300 images for roof class.\n",
      "Processed 2400 images for roof class.\n",
      "Processed 2500 images for roof class.\n",
      "Processed 2600 images for roof class.\n",
      "Processed 2700 images for roof class.\n",
      "Processed 2800 images for roof class.\n",
      "Processed 2900 images for roof class.\n",
      "Processed 3000 images for roof class.\n",
      "Processed 3048 images for roof class in 60.2249019146 seconds.\n",
      "Processed 100 images for nonroof class.\n",
      "Processed 200 images for nonroof class.\n",
      "Processed 300 images for nonroof class.\n",
      "Processed 400 images for nonroof class.\n",
      "Processed 500 images for nonroof class.\n",
      "Processed 600 images for nonroof class.\n",
      "Processed 700 images for nonroof class.\n",
      "Processed 800 images for nonroof class.\n",
      "Processed 900 images for nonroof class.\n",
      "Processed 1000 images for nonroof class.\n",
      "Processed 1100 images for nonroof class.\n",
      "Processed 1200 images for nonroof class.\n",
      "Processed 1300 images for nonroof class.\n",
      "Processed 1400 images for nonroof class.\n",
      "Processed 1500 images for nonroof class.\n",
      "Processed 1600 images for nonroof class.\n",
      "Processed 1700 images for nonroof class.\n",
      "Processed 1800 images for nonroof class.\n",
      "Processed 1900 images for nonroof class.\n",
      "Processed 2000 images for nonroof class.\n",
      "Processed 2100 images for nonroof class.\n",
      "Processed 2200 images for nonroof class.\n",
      "Processed 2300 images for nonroof class.\n",
      "Processed 2400 images for nonroof class.\n",
      "Processed 2500 images for nonroof class.\n",
      "Processed 2600 images for nonroof class.\n",
      "Processed 2700 images for nonroof class.\n",
      "Processed 2800 images for nonroof class.\n",
      "Processed 2900 images for nonroof class.\n",
      "Processed 3000 images for nonroof class.\n",
      "Processed 3058 images for nonroof class in 82.3616139889 seconds.\n",
      "Processed 6106 images total in 147.102118015 seconds.\n"
     ]
    }
   ],
   "source": [
    "sample_dir = '../images/forest/training/'\n",
    "csv_out = '../data/new_training_data.csv'\n",
    "store_image_data(sample_dir, csv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to rewrite `import_image_data` to load the training back into our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonroof' 'roof']\n",
      "Got class labels for 6106 training data points.\n",
      "Got feature vectors for 6106 training data points.\n",
      "(6106, 372)\n",
      "(6106, 48)\n",
      "(6106, 324)\n"
     ]
    }
   ],
   "source": [
    "csv_in = '../data/new_training_data.csv'\n",
    "(features, colors, hogs, labels, label_encoder) = \\\n",
    "                import_image_data(csv_in, display=True)\n",
    "print features.shape\n",
    "print colors.shape\n",
    "print hogs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure none of features are nan\n",
    "np.nonzero(np.isnan(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to test the new features out and see if they're as descriptive as the old ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall classification accuracy: 0.784557907846\n",
      "Took 2.72087001801 seconds.\n",
      "1606 test examples: 835 positive, 771 negative\n",
      "Predicted: 835 positive, 771 negative.\n",
      "Prediction results:\n",
      "    662 true positive, 173 false positive\n",
      "    598 true negative, 173 false negative\n",
      "Roof accuracy: 0.792814371257\n",
      "Nonroof accuracy: 0.775616083009\n"
     ]
    }
   ],
   "source": [
    "# Mix up the data\n",
    "perm = np.random.permutation(labels.size)\n",
    "features = features[perm]\n",
    "labels = labels[perm]\n",
    "t0 = time.time()\n",
    "\n",
    "# Train classifier\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "'''\n",
    "clf1 = ensemble.RandomForestClassifier()\n",
    "clf = ensemble.AdaBoostClassifier(base_estimator=clf1, n_estimators=50,\n",
    "                                  random_state=0)\n",
    "'''\n",
    "# Training on training examples\n",
    "num_train = 4500\n",
    "clf.fit(features[:num_train], labels[:num_train])\n",
    "accuracy = clf.score(features[num_train:], labels[num_train:])\n",
    "print 'Overall classification accuracy: {}'.format(accuracy)\n",
    "t1 = time.time()\n",
    "print 'Took {} seconds.'.format(t1-t0)\n",
    "\n",
    "# Figuring out the true/false positive/negative rates\n",
    "y_hat = clf.predict(features[num_train:])\n",
    "y = labels[num_train:]\n",
    "num_test = y_hat.shape[0]\n",
    "positive = sum(y)\n",
    "negative = num_test - positive\n",
    "print '{} test examples: {} positive, {} negative'.format(num_test,\n",
    "                                                     positive, negative)\n",
    "positive_hat = sum(y_hat)\n",
    "negative_hat = num_test - positive_hat\n",
    "print 'Predicted: {} positive, {} negative.'.format(positive_hat,\n",
    "                                                   negative_hat)\n",
    "# Different types of mistakes:\n",
    "# 0 = correct, -1 = false positive, 1 = false negative\n",
    "mistakes = y - y_hat\n",
    "false_neg = mistakes > 0\n",
    "false_pos = mistakes < 0\n",
    "false_neg_count = sum(false_neg)\n",
    "false_pos_count = sum(false_pos)\n",
    "true_pos = positive_hat - false_pos_count\n",
    "true_neg = negative_hat - false_neg_count\n",
    "print 'Prediction results:'\n",
    "print '    {} true positive, {} false positive'.format(true_pos,\n",
    "                                                      false_pos_count)\n",
    "print '    {} true negative, {} false negative'.format(true_neg,\n",
    "                                                      false_neg_count)\n",
    "print 'Roof accuracy: {}'.format(float(true_pos) / positive)\n",
    "print 'Nonroof accuracy: {}'.format(float(true_neg) / negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on 52 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from forest import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/classify/input/'\n",
    "csv_in = '../data/new_training_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonroof' 'roof']\n",
      "Got class labels for 6106 training data points.\n",
      "Got feature vectors for 6106 training data points.\n"
     ]
    }
   ],
   "source": [
    "classifier = train_forest_classifier(csv_in, boost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0: 17\n",
      "Image 1: 0\n",
      "Image 2: 11\n",
      "Image 3: 0\n",
      "Image 4: 2\n",
      "Image 5: 14\n",
      "Image 6: 9\n",
      "Image 7: 6\n",
      "Image 8: 0\n",
      "Image 9: 0\n",
      "Image 10: 7\n",
      "Image 11: 0\n",
      "Image 12: 9\n",
      "Image 13: 0\n",
      "Image 14: 0\n",
      "Image 15: 1\n",
      "Image 16: 6\n",
      "Image 17: 1\n",
      "Image 18: 4\n",
      "Image 19: 0\n",
      "Image 20: 0\n",
      "Image 21: 2\n",
      "Image 22: 17\n",
      "Image 23: 17\n",
      "Image 24: 12\n",
      "Image 25: 6\n",
      "Image 26: 0\n",
      "Image 27: 0\n",
      "Image 28: 1\n",
      "Image 29: 7\n",
      "Image 30: 1\n",
      "Image 31: 2\n",
      "Image 32: 9\n",
      "Image 33: 9\n",
      "Image 34: 0\n",
      "Image 35: 7\n",
      "Image 36: 2\n",
      "Image 37: 14\n",
      "Image 38: 0\n",
      "Image 39: 2\n",
      "Image 40: 11\n",
      "Image 41: 9\n",
      "Image 42: 3\n",
      "Image 43: 1\n",
      "Image 44: 0\n",
      "Image 45: 7\n",
      "Image 46: 0\n",
      "Image 47: 0\n",
      "Image 48: 3\n",
      "Image 49: 0\n",
      "Image 50: 0\n",
      "Image 51: 15\n",
      "Took 17.0323619843 seconds to process 52 images.\n",
      "0.327545422774 seconds per image.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    # Load in image and one to annotate\n",
    "    image = cv2.imread(image_fn)\n",
    "    # Print roof counts\n",
    "    roof_count = count_roofs(image, classifier)\n",
    "    print 'Image {}: {}'.format(count, roof_count)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Took {} seconds to process {} images.'.format((t1-t0), count)\n",
    "print '{} seconds per image.'.format((t1-t0)/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the non-custom HOG features results in less accurate roof detection than before, but with much improved speed (~3 images/second).\n",
    "\n",
    "The next step is to write a function that will find the roofs in ALL of the images in a folder and save their roof counts in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from forest import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/classify/input/'\n",
    "csv_in = '../data/new_training_data.csv'\n",
    "csv_out = '../images/forest/classify/input_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonroof' 'roof']\n",
      "Got class labels for 6106 training data points.\n",
      "Got feature vectors for 6106 training data points.\n",
      "Stored roof counts for 52 images in 15.023291111 seconds.\n",
      "CPU times: user 19.1 s, sys: 57.3 ms, total: 19.2 s\n",
      "Wall time: 19.2 s\n"
     ]
    }
   ],
   "source": [
    "%time batch_count_roofs(in_dir, csv_in, csv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet, now I can count the roofs in a whole folder of images at once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

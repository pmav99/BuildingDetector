{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "In this notebook, I'm going to try getting rid of the template matching step of my building finder and instead just have a sliding window that checks to see if there are roofs in each window. Doing it this way (with constant candidate patch sizes), I can also use the built-in HOG features.\n",
    "\n",
    "The first step will be to grab a bunch of image samples and pull out roof (positive) and nonroof (negative) training examples. Hopefully we can get a somewhat balanced training set.\n",
    "\n",
    "The next step will be to get a set of representative images randomly sampled from the DHS locations and to try to classify each window within them. In this step, it will be useful to save the image patches classified as roofs and nonroofs in separate folders so that we can identify cases where misclassification occurred. We can then add these hard cases to the training set and hopefully improve our classification algorithm.\n",
    "\n",
    "We can repeat this process until we (hopefully) get a good image patch roof/nonroof classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import urllib\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from features import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1 images from DHS cell 100498 in 0.314904928207 seconds.\n",
      "Sampled 1 images from DHS cell 101916 in 0.340812921524 seconds.\n",
      "Sampled 1 images from DHS cell 101940 in 0.308054208755 seconds.\n",
      "Sampled 1 images from DHS cell 101946 in 0.317348003387 seconds.\n",
      "Sampled 1 images from DHS cell 101976 in 0.331739902496 seconds.\n",
      "Sampled 1 images from DHS cell 103383 in 0.33606004715 seconds.\n",
      "Sampled 1 images from DHS cell 104103 in 0.327265977859 seconds.\n",
      "Sampled 1 images from DHS cell 106271 in 0.463610887527 seconds.\n",
      "Sampled 1 images from DHS cell 106977 in 0.297461032867 seconds.\n",
      "Sampled 1 images from DHS cell 106991 in 0.362490177155 seconds.\n",
      "Sampled 1 images from DHS cell 109148 in 0.330276966095 seconds.\n",
      "Sampled 1 images from DHS cell 110577 in 0.322143793106 seconds.\n",
      "Sampled 1 images from DHS cell 110578 in 0.329143047333 seconds.\n",
      "Sampled 1 images from DHS cell 111296 in 0.366881132126 seconds.\n",
      "Sampled 1 images from DHS cell 111297 in 0.325039863586 seconds.\n",
      "Sampled 1 images from DHS cell 112736 in 0.402367830276 seconds.\n",
      "Sampled 1 images from DHS cell 119239 in 0.389881134033 seconds.\n",
      "Sampled 1 images from DHS cell 119959 in 0.341902971268 seconds.\n",
      "Sampled 1 images from DHS cell 123511 in 0.41277885437 seconds.\n",
      "Sampled 1 images from DHS cell 123560 in 0.316881895065 seconds.\n",
      "Sampled 1 images from DHS cell 124259 in 0.311995029449 seconds.\n",
      "Sampled 1 images from DHS cell 124979 in 0.301486968994 seconds.\n",
      "Sampled 1 images from DHS cell 126427 in 0.248327970505 seconds.\n",
      "Sampled 1 images from DHS cell 127100 in 0.296351909637 seconds.\n",
      "Sampled 1 images from DHS cell 127141 in 0.353507995605 seconds.\n",
      "Sampled 1 images from DHS cell 127859 in 0.234032869339 seconds.\n",
      "Sampled 1 images from DHS cell 127874 in 0.319984197617 seconds.\n",
      "Sampled 1 images from DHS cell 127875 in 0.340662956238 seconds.\n",
      "Sampled 1 images from DHS cell 128594 in 0.297167062759 seconds.\n",
      "Sampled 1 images from DHS cell 128595 in 0.415697813034 seconds.\n",
      "Sampled 1 images from DHS cell 129313 in 0.309138059616 seconds.\n",
      "Sampled 1 images from DHS cell 130746 in 0.315387964249 seconds.\n",
      "Sampled 1 images from DHS cell 135740 in 0.189623117447 seconds.\n",
      "Sampled 1 images from DHS cell 135744 in 0.191528081894 seconds.\n",
      "Sampled 1 images from DHS cell 136453 in 0.296191930771 seconds.\n",
      "Sampled 1 images from DHS cell 136454 in 0.352235078812 seconds.\n",
      "Sampled 1 images from DHS cell 136455 in 0.361223936081 seconds.\n",
      "Sampled 1 images from DHS cell 136456 in 0.320709943771 seconds.\n",
      "Sampled 1 images from DHS cell 136457 in 0.325922012329 seconds.\n",
      "Sampled 1 images from DHS cell 137152 in 0.342066049576 seconds.\n",
      "Sampled 1 images from DHS cell 137157 in 0.214059114456 seconds.\n",
      "Sampled 1 images from DHS cell 137172 in 0.332995891571 seconds.\n",
      "Sampled 1 images from DHS cell 137173 in 0.344295024872 seconds.\n",
      "Sampled 1 images from DHS cell 137174 in 0.317945957184 seconds.\n",
      "Sampled 1 images from DHS cell 137175 in 0.291379928589 seconds.\n",
      "Sampled 1 images from DHS cell 137176 in 0.372917175293 seconds.\n",
      "Sampled 1 images from DHS cell 137177 in 0.324683189392 seconds.\n",
      "Sampled 1 images from DHS cell 137879 in 0.424996137619 seconds.\n",
      "Sampled 1 images from DHS cell 137880 in 0.338303089142 seconds.\n",
      "Sampled 1 images from DHS cell 137892 in 0.351092815399 seconds.\n",
      "Sampled 1 images from DHS cell 137893 in 0.336416006088 seconds.\n",
      "Sampled 1 images from DHS cell 137894 in 0.384188890457 seconds.\n",
      "Sampled 1 images from DHS cell 137895 in 0.324919223785 seconds.\n",
      "Sampled 1 images from DHS cell 137896 in 0.307423830032 seconds.\n",
      "Sampled 1 images from DHS cell 138597 in 0.347908973694 seconds.\n",
      "Sampled 1 images from DHS cell 138599 in 0.350673913956 seconds.\n",
      "Sampled 1 images from DHS cell 138600 in 0.341466903687 seconds.\n",
      "Sampled 1 images from DHS cell 138602 in 0.354264974594 seconds.\n",
      "Sampled 1 images from DHS cell 138611 in 0.183943033218 seconds.\n",
      "Sampled 1 images from DHS cell 138612 in 0.187767982483 seconds.\n",
      "Sampled 1 images from DHS cell 138613 in 0.334267854691 seconds.\n",
      "Sampled 1 images from DHS cell 138614 in 0.328392982483 seconds.\n",
      "Sampled 1 images from DHS cell 138615 in 0.338803052902 seconds.\n",
      "Sampled 1 images from DHS cell 139317 in 0.300364971161 seconds.\n",
      "Sampled 1 images from DHS cell 139326 in 0.3504550457 seconds.\n",
      "Sampled 1 images from DHS cell 139327 in 0.302448034286 seconds.\n",
      "Sampled 1 images from DHS cell 139328 in 0.323198080063 seconds.\n",
      "Sampled 1 images from DHS cell 139329 in 0.3088722229 seconds.\n",
      "Sampled 1 images from DHS cell 139332 in 0.16401386261 seconds.\n",
      "Sampled 1 images from DHS cell 139333 in 0.320415973663 seconds.\n",
      "Sampled 1 images from DHS cell 139336 in 0.317822933197 seconds.\n",
      "Sampled 1 images from DHS cell 140030 in 0.338711023331 seconds.\n",
      "Sampled 1 images from DHS cell 140047 in 0.369621992111 seconds.\n",
      "Sampled 1 images from DHS cell 140048 in 0.742947101593 seconds.\n",
      "Sampled 1 images from DHS cell 140049 in 0.324589014053 seconds.\n",
      "Sampled 1 images from DHS cell 140051 in 0.325776100159 seconds.\n",
      "Sampled 1 images from DHS cell 140750 in 0.183202981949 seconds.\n",
      "Sampled 1 images from DHS cell 140769 in 0.336931943893 seconds.\n",
      "Sampled 1 images from DHS cell 140770 in 0.333997964859 seconds.\n",
      "Sampled 1 images from DHS cell 140771 in 0.397030115128 seconds.\n",
      "Sampled 1 images from DHS cell 140773 in 0.320351839066 seconds.\n",
      "Sampled 1 images from DHS cell 140774 in 0.382009983063 seconds.\n",
      "Sampled 1 images from DHS cell 141489 in 0.30465388298 seconds.\n",
      "Sampled 1 images from DHS cell 141490 in 0.432116985321 seconds.\n",
      "Sampled 1 images from DHS cell 142210 in 0.283550024033 seconds.\n",
      "Sampled 1 images from DHS cell 142935 in 0.476649999619 seconds.\n",
      "Sampled 1 images from DHS cell 142936 in 0.317348957062 seconds.\n",
      "Sampled 1 images from DHS cell 142946 in 0.297582864761 seconds.\n",
      "Sampled 1 images from DHS cell 142998 in 0.307710886002 seconds.\n",
      "Sampled 1 images from DHS cell 143613 in 0.297991991043 seconds.\n",
      "Sampled 1 images from DHS cell 143654 in 0.338037967682 seconds.\n",
      "Sampled 1 images from DHS cell 143666 in 0.330949068069 seconds.\n",
      "Sampled 1 images from DHS cell 143668 in 0.415310144424 seconds.\n",
      "Sampled 1 images from DHS cell 144379 in 0.328680038452 seconds.\n",
      "Sampled 1 images from DHS cell 145096 in 0.295392990112 seconds.\n",
      "Sampled 1 images from DHS cell 145816 in 0.298403978348 seconds.\n",
      "Sampled 1 images from DHS cell 147254 in 0.311501026154 seconds.\n",
      "Sampled 1 images from DHS cell 147258 in 0.300970077515 seconds.\n",
      "Sampled 1 images from DHS cell 147271 in 0.288270950317 seconds.\n",
      "Sampled 1 images from DHS cell 147944 in 0.305598020554 seconds.\n",
      "Done. Sampled 100 images total.\n"
     ]
    }
   ],
   "source": [
    "# Import and preview csv data\n",
    "dhs_fn = '../data/IMR1990-2000_NL1992-2012_thresh100.csv'\n",
    "dhs_data = pd.read_csv(dhs_fn)\n",
    "\n",
    "# Samples to take from each location\n",
    "samples = 1\n",
    "# Number of locations to sample from\n",
    "locations = 100\n",
    "\n",
    "# Output image directory\n",
    "out_dir = '../images/forest/samples/'\n",
    "out_csv = '../data/dhs_image_metadata.csv'\n",
    "\n",
    "# Create DataFrame for image metadata\n",
    "image_data = pd.DataFrame(columns=['image', 'cellid', 'cell_lat',\n",
    "                                  'cell_lon', 'lat', 'lon'])\n",
    "\n",
    "# Sampling images from DHS locations\n",
    "for index, row in dhs_data.iterrows():\n",
    "    if index + 1 > locations:\n",
    "        break\n",
    "    \n",
    "    cell_id = int(row['cellid'])\n",
    "    cell_lat = row['lat']\n",
    "    cell_lon = row['lon']\n",
    "    # Sample images\n",
    "    image_data = sample_dhs(image_data, cell_id, cell_lat, cell_lon,\n",
    "                           samples, out_dir)\n",
    "\n",
    "# Save image metadata to csv\n",
    "image_data.set_index('image', inplace=True)\n",
    "image_data.to_csv(out_csv)\n",
    "\n",
    "print 'Done. Sampled {} images total.'.format(samples * locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 17 images in 0.0105409622192 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Rename images first\n",
    "in_dir = '../images/forest/samples/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = in_dir + str(count) + '.png'\n",
    "    os.rename(image_fn, out_fn)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Renamed {} images in {} seconds.'.format(count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have some images, we need to split them up into 81 (9x9) 80x80 pixel patches. Let's write a few functions to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_patch(image, out_fn, x=0, y=0, width=80, height=80):\n",
    "    \"\"\"\n",
    "    This function saves a specified image patch from a image.\n",
    "    :param image: Input 3-channel image\n",
    "    :param out_fn: Filename for output image patch\n",
    "    :param x: Top left x-coordinate of image patch\n",
    "    :param y: Top left y-coordinate of image patch\n",
    "    :param width: Width in pixels of image patch\n",
    "    :param height: Height in pixels of image patch\n",
    "    \"\"\"\n",
    "    patch = image[x:x+width, y:y+width, :]\n",
    "    cv2.imwrite(out_fn, patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_patches(image_fn, out_dir, width=80, height=80):\n",
    "    \"\"\"\n",
    "    This function takes each input image and saves however many image\n",
    "    patches of the specified size as can fit in the original image. \n",
    "    Image patches are offset by half the height and width specified.\n",
    "    :param image_fn: Input image filename\n",
    "    :param out_dir: Folder where image patches will be saved\n",
    "    :param width: Width of each image patch\n",
    "    :param height: Height of each image patch\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_fn)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            out_fn = (out_dir + os.path.basename(image_fn)[:-4] + '_' +\n",
    "                      str(count) + '.png')\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y, width,\n",
    "                       height)\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that we can save 81 patches from any image, let's go through our samples and save 81 patches for each of them to separate into positive and negative training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved patches for 142 images.\n"
     ]
    }
   ],
   "source": [
    "# Save patches from all sample images\n",
    "in_dir = '../images/forest/samples/'\n",
    "out_dir = '../images/forest/patches/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    save_patches(image_fn, out_dir)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Saved patches for {} images in {} seconds.'.format(count,(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 722 images in 2.19266605377 seconds.\n",
      "Copied 1058 images in 3.18941497803 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Rename images first\n",
    "in_dir = '../images/forest/training/roof/'\n",
    "out_dir = '../images/forest/training/new_roof/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = out_dir + str(count) + '.png'\n",
    "    shutil.copyfile(image_fn, out_fn)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Copied {} images in {} seconds.'.format(count, (t1-t0))\n",
    "# Rename images first\n",
    "in_dir = '../images/forest/training/nonroof/'\n",
    "out_dir = '../images/forest/training/new_nonroof/'\n",
    "t0 = time.time()\n",
    "count = 0\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    out_fn = out_dir + str(count) + '.png'\n",
    "    shutil.copyfile(image_fn, out_fn)\n",
    "    count += 1\n",
    "t1 = time.time()\n",
    "print 'Copied {} images in {} seconds.'.format(count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from training examples\n",
    "Ok, now that we've organized those patches into roof and nonroof classes, let's first use the feature extractor functions that we wrote earlier and see how well it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 722 images for roof class.\n",
      "Processed 1058 images for nonroof class.\n",
      "Processed 1780 images total in 149.869764805 seconds.\n"
     ]
    }
   ],
   "source": [
    "sample_dir = '../images/forest/training/'\n",
    "csv_out = '../data/forest_training_data.csv'\n",
    "store_image_data(sample_dir, csv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load those features back into the workspace so that we can use them for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nonroof' 'roof']\n",
      "Got class labels for 1780 training data points.\n",
      "Got feature vectors for 1780 training data points.\n"
     ]
    }
   ],
   "source": [
    "csv_in = '../data/forest_training_data.csv'\n",
    "(features, colors, hogs, mags, labels, label_encoder) = \\\n",
    "                import_image_data(csv_in, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well a random forest classifier does on these training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mix up the data\n",
    "perm = np.random.permutation(labels.size)\n",
    "features = features[perm]\n",
    "labels = labels[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall classification accuracy: 0.917948717949\n",
      "Took 0.134510993958 seconds.\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "# Training on training examples\n",
    "num_train = 1000\n",
    "clf.fit(features[:num_train], labels[:num_train])\n",
    "accuracy = clf.score(features[num_train:], labels[num_train:])\n",
    "print 'Overall classification accuracy: {}'.format(accuracy)\n",
    "t1 = time.time()\n",
    "print 'Took {} seconds.'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780 test examples: 312 positive, 468 negative\n",
      "Predicted: 288 positive, 492 negative.\n",
      "Prediction results:\n",
      "    268 true positive, 20 false positive\n",
      "    448 true negative, 44 false negative\n",
      "Roof accuracy: 0.858974358974\n",
      "Nonroof accuracy: 0.957264957265\n"
     ]
    }
   ],
   "source": [
    "# Figuring out the true/false positive/negative rates\n",
    "y_hat = clf.predict(features[num_train:])\n",
    "y = labels[num_train:]\n",
    "num_test = y_hat.shape[0]\n",
    "positive = sum(y)\n",
    "negative = num_test - positive\n",
    "print '{} test examples: {} positive, {} negative'.format(num_test,\n",
    "                                                     positive, negative)\n",
    "positive_hat = sum(y_hat)\n",
    "negative_hat = num_test - positive_hat\n",
    "print 'Predicted: {} positive, {} negative.'.format(positive_hat,\n",
    "                                                   negative_hat)\n",
    "# Different types of mistakes:\n",
    "# 0 = correct, -1 = false positive, 1 = false negative\n",
    "mistakes = y - y_hat\n",
    "false_neg = mistakes > 0\n",
    "false_pos = mistakes < 0\n",
    "false_neg_count = sum(false_neg)\n",
    "false_pos_count = sum(false_pos)\n",
    "true_pos = positive_hat - false_pos_count\n",
    "true_neg = negative_hat - false_neg_count\n",
    "print 'Prediction results:'\n",
    "print '    {} true positive, {} false positive'.format(true_pos,\n",
    "                                                      false_pos_count)\n",
    "print '    {} true negative, {} false negative'.format(true_neg,\n",
    "                                                      false_neg_count)\n",
    "print 'Roof accuracy: {}'.format(float(true_pos) / positive)\n",
    "print 'Nonroof accuracy: {}'.format(float(true_neg) / negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks extremely promising! Let's get some new random DHS images and try to classify the image patches within them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifier\n",
    "It seems that this might actually result in a good roof detector! Let's get a new set of DHS images and see how it does classifying all the image patches. We can see where it makes mistakes and then add those \"hard\" examples to the training set--hopefully that will improve the classifier going forward.\n",
    "\n",
    "Ok, we've use the code above to get a new set of DHS images. Let's break those up into 81 image patches each and classify each of them, saving them into separate roof and nonroof folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/samples/'\n",
    "roof_dir = '../images/forest/classify/roof/'\n",
    "nonroof_dir = '../images/forest/classify/nonroof/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='auto', criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest classifier on all training examples\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying patches in image 1.\n",
      "Classifying patches in image 2.\n",
      "Classifying patches in image 3.\n",
      "Classifying patches in image 4.\n",
      "Classifying patches in image 5.\n",
      "Classifying patches in image 6.\n",
      "Classifying patches in image 7.\n",
      "Classifying patches in image 8.\n",
      "Classifying patches in image 9.\n",
      "Classifying patches in image 10.\n",
      "Classifying patches in image 11.\n",
      "Classifying patches in image 12.\n",
      "Classifying patches in image 13.\n",
      "Classifying patches in image 14.\n",
      "Classifying patches in image 15.\n",
      "Classifying patches in image 16.\n",
      "Classifying patches in image 17.\n",
      "Classifying patches in image 18.\n",
      "Classifying patches in image 19.\n",
      "Classifying patches in image 20.\n",
      "Classifying patches in image 21.\n",
      "Classifying patches in image 22.\n",
      "Classifying patches in image 23.\n",
      "Classifying patches in image 24.\n",
      "Classifying patches in image 25.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-b08dccf859f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m                                     axis=0)\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# Classify image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# 0 = nonroof, 1 = roof\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mnonroof_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;31m# ensure_2d=False because there are actually unit test checking we fail\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[1;31m# for 1d.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[0;32m    350\u001b[0m                              array.ndim)\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/afs/cs.stanford.edu/u/nealjean/.local/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     50\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     51\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 52\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Classify each image patch\n",
    "width = 80\n",
    "height = 80\n",
    "image_count = 0\n",
    "t0 = time.time()\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    # Load in image\n",
    "    image = cv2.imread(image_fn)\n",
    "    image_count += 1\n",
    "    print 'Classifying patches in image {}.'.format(image_count)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            # Get feature vector from image patch\n",
    "            patch = image[top_left_x:top_left_x+width,\n",
    "                          top_left_y:top_left_y+width, :]\n",
    "            color = calc_color_hist(patch)\n",
    "            color = color.flatten()\n",
    "            (hog, hog_bins, magnitude_hist, magnitude_bins,\n",
    "             max_magnitude) = compute_hog(patch)\n",
    "            feature = np.concatenate((color, hog, magnitude_hist),\n",
    "                                    axis=0)\n",
    "            # Classify image\n",
    "            predict = clf.predict(feature)[0] # 0 = nonroof, 1 = roof\n",
    "            probs = clf.predict_proba(feature)[0]\n",
    "            nonroof_prob = probs[0]\n",
    "            roof_prob = probs[1]\n",
    "            # Decide where to save image patch\n",
    "            if predict == 1:\n",
    "                out_fn = (roof_dir + os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '_' + str(roof_prob) +  '.png')\n",
    "            else:\n",
    "                out_fn = (nonroof_dir + os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '_' + str(nonroof_prob) + '.png')\n",
    "            # Save image patch\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y,\n",
    "                           width, height)\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)\n",
    "t1 = time.time()\n",
    "print 'Classified image patches for {} images in {} seconds.'.format(\n",
    "                                        image_count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this a couple more times to get more hard training examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Writing annotated classified images\n",
    "Our next step will be to save annotated versions of our classified images so that we can visually inspect them to see how well our classifier is doing. Some things that we want to do here are:\n",
    "\n",
    "- Draw bounding boxes around patches identified as roofs\n",
    "- Use non-maximum suppression to keep only the best patch out of overlapping patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Draw all bounding boxes around patches classified as roofs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '../images/forest/samples/'\n",
    "roof_dir = '../images/forest/classify/roof/'\n",
    "nonroof_dir = '../images/forest/classify/nonroof/'\n",
    "annotated_dir = '../images/forest/classify/annotated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='auto', criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train random forest classifier on all training examples\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=50, random_state=0,\n",
    "                                      class_weight='auto')\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying patches in image 1.\n",
      "Classifying patches in image 2.\n",
      "Classifying patches in image 3.\n",
      "Classifying patches in image 4.\n",
      "Classifying patches in image 5.\n",
      "Classifying patches in image 6.\n",
      "Classifying patches in image 7.\n",
      "Classifying patches in image 8.\n",
      "Classifying patches in image 9.\n",
      "Classifying patches in image 10.\n",
      "Classifying patches in image 11.\n",
      "Classifying patches in image 12.\n",
      "Classifying patches in image 13.\n",
      "Classifying patches in image 14.\n",
      "Classifying patches in image 15.\n",
      "Classifying patches in image 16.\n",
      "Classifying patches in image 17.\n",
      "Classified image patches for 17 images in 114.938821077 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Classify each image patch\n",
    "width = 80\n",
    "height = 80\n",
    "image_count = 0\n",
    "font = cv2.FONT_HERSHEY_TRIPLEX\n",
    "annotation_color = (0,255,0) # green\n",
    "t0 = time.time()\n",
    "for image_fn in glob.glob(in_dir + '*'):\n",
    "    # Load in image and one to annotate\n",
    "    image = cv2.imread(image_fn)\n",
    "    image_out = cv2.imread(image_fn)\n",
    "    image_count += 1\n",
    "    print 'Classifying patches in image {}.'.format(image_count)\n",
    "    # Get dimensions of input image\n",
    "    max_x, max_y = image.shape[:2]\n",
    "    # Pull image patches as long as they fit in the image\n",
    "    count = 0\n",
    "    top_left_x = 0\n",
    "    while (top_left_x + (height - 1) < max_x):\n",
    "        top_left_y = 0\n",
    "        while (top_left_y + (width - 1) < max_y):\n",
    "            # Get feature vector from image patch\n",
    "            patch = image[top_left_x:top_left_x+height,\n",
    "                          top_left_y:top_left_y+width, :]\n",
    "            color = calc_color_hist(patch)\n",
    "            color = color.flatten()\n",
    "            (hog, hog_bins, magnitude_hist, magnitude_bins,\n",
    "             max_magnitude) = compute_hog(patch)\n",
    "            feature = np.concatenate((color, hog, magnitude_hist),\n",
    "                                    axis=0)\n",
    "            # Classify image patch\n",
    "            predict = clf.predict(feature)[0] # 0 = nonroof, 1 = roof\n",
    "            probs = clf.predict_proba(feature)[0]\n",
    "            nonroof_prob = probs[0]\n",
    "            roof_prob = probs[1]\n",
    "            # Annotate image when patches classified as roofs\n",
    "            if predict == 1:\n",
    "                cv2.putText(image_out, str(roof_prob),\n",
    "                            (top_left_x + height/2, top_left_y + width/2),\n",
    "                            font, 0.5, annotation_color, thickness=1,\n",
    "                            lineType=cv2.CV_AA)\n",
    "                cv2.rectangle(image_out, (top_left_x, top_left_y),\n",
    "                             (top_left_x+height-1, top_left_y+width-1),\n",
    "                             annotation_color)\n",
    "            # Decide where to save image patch\n",
    "            if predict == 1:\n",
    "                out_fn = (roof_dir + str(roof_prob) + '_' +\n",
    "                          os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) +  '.png')\n",
    "            else:\n",
    "                out_fn = (nonroof_dir + str(nonroof_prob) + '_' +\n",
    "                          os.path.basename(image_fn)[:-4] +\n",
    "                          '_' + str(count) + '.png')\n",
    "            # Save image patch\n",
    "            save_patch(image, out_fn, top_left_x, top_left_y,\n",
    "                           width, height)\n",
    "            count += 1\n",
    "            top_left_y += (width/2)\n",
    "        top_left_x += (height/2)\n",
    "    # Save annotated image\n",
    "    image_out_fn = annotated_dir + os.path.basename(image_fn)\n",
    "    cv2.imwrite(image_out_fn, image_out)\n",
    "t1 = time.time()\n",
    "print 'Classified image patches for {} images in {} seconds.'.format(\n",
    "                                        image_count, (t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adding non-maximum suppression\n",
    "What we should do here is save all the locations and probabilities of the patches classified as roofs within each image. Then at the end, we can choose the maximum probability patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
